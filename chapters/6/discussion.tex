% Copyright 2022 Néstor Nápoles López

An \gls{arna} model is more than a neural network
architecture. It should be seen as an end-to-end model that
consists both of machine learning components and musical
decisions. When I started this project, I assumed that the
goal was to create a ``machine learning model'' that would
solve the problem. However, from the beginning, parsing
different datasets in different annotation formats soon made
me realize that domain knowledge was necessary to clean the
datasets, standardize their musical assumptions, define the
chord vocabulary, and guide the design of the network. This
is true for the output of the model as well. For example,
determining the number of classification outputs and how
these are defined, the number of key layers in the musical
analysis, or the spelling of certain Roman numerals (e.g.,
that infamous $\rn{I}\rnsixfour$ vs $\rn{V}\rnsixfour$). All
of these aspects shape the design of an \gls{arna} model.

Out of the ideas explored in the comparison process, the
\gls{natem} algorithm seems particularly promising to me in
future experiments. The design of tonal music analysis
models is open to the interpretation of the researchers
implementing them. This makes it very difficult to compare
them. Perhaps the distribution of \reftab{rare_chords}
summarizes my current thoughts on the design of the
\gls{arna} models. Even if a model is not competitive in
most metrics, it may be useful in certain situations, or
adopt certain design patterns that result beneficial for
certain types of music or certain types of chords. Thus, it
is worth the effort to search for tools that allow these
models to ``communicate'' with each other, and obtain
feedback on the things they do well and do poorly. Taking
the performance on the $\rnFr$ as an example, noticing the
peculiar intervallic configuration of the chord and
designing a representation that exploits it might be the
difference between recognizing all $\rnFr$ or none. 

\phdfigurefullpage[Comparison of the annotations provided by
a human analyst and various \gls{arna} models. The musical
excerpt is from Haydn's Op. 20 No. 3 - IV, which is part of
the \gls{haydnsun} test set]{haydn_comparison}

In order to complement the quantitative results presented in
this section, \reffig{haydn_comparison} shows a real musical
excerpt analyzed by the various \gls{arna} systems. The
example includes the original human annotations as well, for
reference. The key of the excerpt is $\keyg{}$, which all of
the models except \gls{melisma} predict correctly. In
general, the \gls{melisma} model presents several other
flaws. For example, it misses the initial annotation of the
$\pitchG{}$ minor triad at the beginning of measure 1, and
it shows a long stream of chord annotations in measure 3,
which greatly differs from the harmonic rhythm implied by
the human annotator, or the other \gls{arna} models. After
the \gls{melisma} model, the \textcite{chen2021attend} model
provides better performance. Importantly, all but one of the
annotations (the $\rnIt$ chord on measure 8, beat 2) in
\textcite{chen2021attend} are either tonic or dominant
chords. This speaks of the bias that exists for the Roman
numeral classes that appear more often in the dataset, as
shown in \reftab{rare_chords}. The \textcite{micchi2021deep}
model predicts the same chord as the human annotator in
various instances, however, it seems to struggle with the
segmentation of the chords, as these are often in odd
locations. For example, the quick chord changes in measure
9. The \textcite{mcleod2021modular} model is fairly similar
to the human annotator. Two drawbacks of this model is that
it often predicts the wrong inversion (e.g., measure 1, beat
3; or measure 6, beat 3). In addition to this, it is unable
to recognize the $\rnGer$ chord in measure 8, beat 2. The
proposed model in this dissertation, \gls{augmentednet} is
the only model that seems to be able to recognize that
chord, the $\rnGer$ \gls{augsix} in measure 8. Other models
seem to provide either an $\rnIt$ label, or in the case of
\textcite{mcleod2021modular}, a label of g:$\rn{VI}\rnsix$,
which is a reasonable prediction when ignoring the
C$\musSharp$ in the first violin. However, this intervallic
relationship between E$\musFlat$4 and C$\musSharp$5 is
precisely what characterizes the \gls{augsix} chord. The
prediction of $\rnIt$ here is expected in
\textcite{chen2021attend} and \textcite{micchi2021deep}, as
these models collapse the various flavours of \gls{augsix}
chords into one category, which makes it difficult (perhaps
impossible) to distinguish between $\rnIt$ and $\rnGer$.
Notice also that there are two errors in the human analysis.
In measure 2, beat 1, the analyst indicated a $\rni\rnsix$
label, however, because the viola crosses the violoncello at
that point, the chord is really in root position. All
\gls{arna} models unequivocally get this annotation right.
The second error in the annotation lies in the inversion of
the $\rnGer$ chord, indicated by the annotator as ``root
position''. The \gls{augmentednet} model gets the annotation
in the proper inversion, according to the convention for
chord inversions used in the \gls{romantext}
format.\footnote{This particular set of chords,
\gls{augsix}, often have differing conventions when it comes
to annotating their inversions. }

% The string quartet example features a polyphonic texture,
% which would be difficult to analyze by a simple model
% (e.g., one that assumes the absence)