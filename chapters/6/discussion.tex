% Copyright 2022 Néstor Nápoles López

An \gls{arna} model is more than a neural network
architecture. It should be seen as an end-to-end model that
consists both of machine learning components and musical
decisions. When I started this project, I assumed that the
goal was to create a ``machine learning model'' that would
solve the problem. However, from the beginning, parsing
different datasets in different annotation formats soon made
me realize that domain knowledge was necessary to clean the
datasets, standardize their musical assumptions, define the
chord vocabulary, and guide the design of the network. This
is true for the output of the model as well. For example,
determining the number of classification outputs and how
these are defined, the number of key layers in the musical
analysis, or the spelling of certain Roman numerals (e.g.,
that infamous $\rn{I}\rnsixfour$ vs $\rn{V}\rnsixfour$). All
of these aspects shape the design of an \gls{arna} model.

Out of the ideas explored in the comparison process, the
\gls{natem} algorithm seems particularly promising to me in
future experiments. The design of tonal music analysis
models is open to the interpretation of the researchers
implementing them. This makes it very difficult to compare
them. Perhaps the distribution of \reftab{rare_chords}
summarizes my current thoughts on the design of the
\gls{arna} models. Even if a model is not competitive in
most metrics, it may be useful in certain situations, or
adopt certain design patterns that result beneficial for
certain types of music or certain types of chords. Thus, it
is worth the effort to search for tools that allow these
models to ``communicate'' with each other, and obtain
feedback on the things they do well and do poorly. Taking
the performance on the $\rnFr$ as an example, noticing the
peculiar intervallic configuration of the chord and
designing a representation that exploits it might be the
difference between recognizing all $\rnFr$ or none. 

\phdfigurefullpage[Comparison of the annotations provided by
a human analyst and various \gls{arna} models. The musical
excerpt is from Haydn's Op. 20 No. 3 - IV, a piece in the
\gls{haydnsun} test set. The annotations from the \gls{arna}
models that differ from the human analyst's are marked in
red]{haydn_comparison}

In order to complement the quantitative results presented in
this section, \reffig{haydn_comparison} shows a musical
excerpt analyzed by the various \gls{arna} systems. For
reference, the example also shows the human annotations
(``ground truth'') in the dataset. The key of the excerpt is
$\keyg$, which all of the models except \gls{melisma}
predict correctly. A brief discussion of the results by each
model is presented below. 

\paragraph{Analysis of the Melisma model.}

In general, the \gls{melisma} model seems to present more
flaws than all other \gls{arna} models. The most important
one perhaps being mislabelling the key. After that, it
misses the initial annotation of the tonic triad (m.1, beat
1). The model also has a tendency to provide long streams of
chord annotations (e.g., m. 3), which greatly differs from
the harmonic rhythm suggested by the human annotator or the
other \gls{arna} models. 

\paragraph{Analysis of the Chen and Su model.}

After inspecting the annotations of \gls{melisma}, the
\textcite{chen2021attend} model seems to provide better
predictions. Importantly, all but one of the annotations
(the $\rnIt\rnsix$ chord on m. 8, beat 2) in
\textcite{chen2021attend} are either a tonic or dominant
chord. This speaks about the bias that exists for the Roman
numeral classes that appear more often in the dataset, as
shown in \reftab{rare_chords}. 

\paragraph{Analysis of the Micchi et al. model.}

The \textcite{micchi2021deep} model is less biased towards
tonic and dominant chords. For instance, it recognizes
instances of diminished chords (e.g., m.~2, beat 3). The
model seems to struggle with the chord segmentation, as
chords are often in odd locations. For example, the
precipitated \gls{neapolitan} (m. 9, beat 3), or the two
tonic triads in the previous measure (m. 8, beat 1). This is
perhaps because the musical excerpt features an anacrusis,
which makes it more complicated for the model to predict the
appropriate harmonic rhythm. 

\paragraph{Analysis of the McLeod and Rohrmeier model.}

The \textcite{mcleod2021modular} model does not seem to
suffer from segmentation problems. In general, it also seems
fairly similar to the human annotator. Two drawbacks of this
model is that it often predicts the wrong inversion (e.g.,
m.~1, beat 3; m.~5, beat 3; and m.~6, beat 3). In addition
to this, it is unable to recognize the $\rnGer$ chord in the
musical excerpt (m.~8, beat 2), proposing an annotation of
$\rnVI\rnsix$ instead. The $\rnVI\rnsix$ annotation is a
reasonable one when ignoring the $\pitchCs$ in the first
violin. However, this intervallic relationship between
$\pitchEb$4 and $\pitchCs$5 is precisely what characterizes
the \gls{augsix} chord here. This is perhaps a more subtle
criticism, considering that $\rnGer$ chords occur in less
than 1\% of the annotations, however, one could argue that
it is in these subtleties that an \gls{arna} model could
appear to be better than others, in the eyes of a human
analyst.

\paragraph{Analysis of the AugmentedNet.}

The proposed model in this dissertation, \gls{augmentednet},
provides annotations that are generally similar to the human
analysis. Among the points of discrepancy, the model
predicts a diminished triad of a tonicized minor dominant
(m. 2, beat 4.5), which coincides with the chord and
inversion appearing in the music score, but that was not
annotated by the human analyst. Another discrepancy is a
second instance of the $\rnV\rnseven$ chord in measure 4,
which is considered by the analyst (and the
\textcite{mcleod2021modular} model) as a continuation of the
chord in measure 3. Lastly, the model (as well as all other
models) does not predict the $\rnVI$ chord instance
expressed by the annotator before the \gls{neapolitan} (m.9,
beat 2.5). Yet, most other annotations coincide with the
human analysis.

Notice also two errors in the human analysis. The first one,
in the $\rni\rnsix$ label (m. 2, beat 1), where the analyst
indicated a first inversion. Nonetheless, because the viola
crosses the violoncello at that point, the chord is in fact
in root position. All \gls{arna} models unequivocally get
this annotation correct. The second error in the human
analysis lies in the inversion of the $\rnGer$ chord,
indicated by the annotator as in ``root position.'' The
\gls{augmentednet} model gets the annotation in the proper
inversion, according to the conventions used in the
\gls{romantext} format. This particular set of chords,
\gls{augsix}, are often a source of disagreement regarding
the definition of ``root position.'' In the \gls{romantext}
format, used to compare these annotations, the root of a
$\rnGer$ chord is located an augmented fourth above the
tonic (i.e., $\pitchCs$, in this case). The
\textcite{micchi2021deep} correctly predicts the $\rnGer$
chord, but the inversion suggests $\pitchEb$ as the bass,
which is not the case. This highlights another issue,
because digital \gls{rna} standards often differ in their
digitization approach for certain chords, it is still
challenging to distinguish a model's misperformance from a
mistranslation of the data. Although a meticulous effort was
put in the quantitative evaluation and comparison presented
here, more effort from the community is needed here,
translation between \gls{rna} standards, if there's a desire
for better \gls{arna} models.