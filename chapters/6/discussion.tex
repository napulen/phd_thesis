% Copyright 2022 Néstor Nápoles López

An \gls{arna} model is more than a neural network
architecture. It should be seen as an end-to-end model that
consists both of machine learning components and musical
decisions. When I started this project, I assumed that the
goal was to create a ``machine learning model'' that would
solve the problem. However, from the beginning, parsing
different datasets in different annotation formats soon made
me realize that domain knowledge was necessary to clean the
datasets, standardize their musical assumptions, define the
chord vocabulary, and guide the design of the network. This
is true for the output of the model as well. For example,
determining the number of classification outputs and how
these are defined, the number of key layers in the musical
analysis, or the spelling of certain Roman numerals (e.g.,
that infamous $\rn{I}\rnsixfour$ vs $\rn{V}\rnsixfour$). All
of these aspects shape the design of an \gls{arna} model.

Out of the ideas explored in the comparison process, the
\gls{natem} algorithm seems particularly promising to me in
future experiments. The design of tonal music analysis
models is open to the interpretation of the researchers
implementing them. This makes it very difficult to compare
them. Perhaps the distribution of \reftab{rare_chords}
summarizes my current thoughts on the design of the
\gls{arna} models. Even if a model is not competitive in
most metrics, it may be useful in certain situations, or
adopt certain design patterns that result beneficial for
certain types of music or certain types of chords. Thus, it
is worth the effort to search for tools that allow these
models to ``communicate'' with each other, and obtain
feedback on the things they do well and do poorly. Taking
the performance on the $\rnFr$ as an example, noticing the
peculiar intervallic configuration of the chord and
designing a representation that exploits it might be the
difference between recognizing all $\rnFr$ or none. 

\phdfigurefullpage[Comparison of the annotations provided by
a human analyst and various \gls{arna} models. The musical
excerpt is from Haydn's Op. 20 No. 3 - IV, which is part of
the \gls{haydnsun} test set]{haydn_comparison}

In order to complement the quantitative results presented in
this section, \reffig{haydn_comparison} shows a real musical
excerpt analyzed by the various \gls{arna} systems. The
example includes the original human annotations as well, for
reference.

The key of the excerpt is $\keyg{}$, which all of the models
except \gls{melisma} predict correctly. In general, the
\gls{melisma} model presents several evident flaws. For
example, it misses the initial annotation of $\keyg$ at the
beginning of measure 1, and it shows a long stream of chord
annotations in measure 3, which greatly differs from the
harmonic rhythm implied by the human annotator, or the other
\gls{arna} models. After the \gls{melisma} model, the
\textcite{chen2021attend} model provides a better
performance. Importantly, all but one of the annotations
(the $\rnIt$ chord on measure 8, beat 2) are either tonic or
dominant chords in this model. This speaks of the bias that
exists for the classes that appear more often in the
dataset, as shown in \reftab{rare_chords}.

% the proposed model is the only one able to detect the
% \gls{augsix} chord in the excerpt. There, the model by
% \textcite{mcleod2021modular} proposes a label of
% g:$\rn{VI}\rnsix$, which is a reasonable guess when
% ignoring the C$\musSharp$ in the first violin. However,
% this intervallic relationship between E$\musFlat$4 and
% C$\musSharp$5 is precisely the indication that would lead
% an analyst to label the chord as an \gls{augsix}.

% The string quartet example features a polyphonic texture,
% which would be difficult to analyze by a simple model
% (e.g., one that assumes the absence)