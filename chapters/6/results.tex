% Copyright 2022 Néstor Nápoles López

This section introduces the results of comparing the five
\gls{arna} models. Three evaluations were performed. The
first evaluation compared the inference time of the models
in the test set. That is, the time it took for the models to
output their predictions. The second evaluation experiment
measured the average accuracy obtained for the ``individual
tasks'' of \gls{pcset} $\elpcset$, key $\elkey$ and
inversion $\elinv$, compared to the same labels in the
ground truth. The third evaluation experiment measured the
accuracy of the numerators $\elnum$ obtained by the models
against the ground-truth. 

% Unfortunately, the models are often defined in different
% ways and solving similar but not identical problems,
% making a one-to-one comparison difficult. For example, the
% model by \textcite{mcleod2021modular} considers one layer
% of key analysis, whereas this model and others consider
% two layers (i.e., modulations and tonicizations). However,
% when possible, a comparison of individual tasks is
% performed. Using a reduced vocabulary of Roman numerals,
% it is possible to ``collapse'' several classes of chords
% into a single string representation. Using this approach,
% the models can be compared for their Roman numeral
% annotations directly, instead of the classification
% problems. This approach is particularly useful when
% assessing the performance of the models on difficult
% chords, that appear only sporadically in the dataset. The
% performance on such difficult chords is presented in
% \refsubsubsec{performanceondifficultchords}.
