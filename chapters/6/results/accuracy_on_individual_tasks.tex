% Copyright 2022 Néstor Nápoles López

\phdtable[Comparison of the accuracy achieved by the five
models on the individual tasks $\elpcset$, $\elkey$, and
$\elinv$]{comparison}

The accuracy of the models on the individual tasks of
\gls{pcset} $\elpcset$, key $\elkey$, and inversion $\elinv$
is shown in \reftab{comparison}. In this evaluation,
\gls{melisma} model seems to be the worst-performing model.
Surprisingly, the performance of two deep learning
approaches, \textcite{mcleod2021modular,chen2021attend}, is
not too far ahead of \gls{melisma}. Given the nature of
\gls{romantext} files, which have discrete chord annotations
located at measure and beat positions, it is possible that
the lower performance shown in these models is heavily
penalizing the chord segmentation of the models. If the
predictions are correct but misaligned with the annotations
of the ground truth, it is possible that the performance of
the models appears to be worse than it would be perceived in
a different evaluation workflow. The
\textcite{micchi2021deep} and \gls{augmentednet} models have
a similar performance, with \gls{augmentednet} slightly
ahead on the $\elpcset$ and $\elkey$ tasks.

