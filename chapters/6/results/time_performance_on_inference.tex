% Copyright 2022 Néstor Nápoles López

The time that each model took to compute its predictions on
the test set of 94 \gls{musicxml} files was
measured.\footnote{In the case of \gls{melisma} the time it
took to compute its predictions over the set of 94
\gls{humkern} files.} In order to run the measurement, each
of the files was processed as an independent process in the
operating system, running the model with the input and
output arguments for the file in turn. For the models based
on deep learning approaches, no \gls{gpu} was used to
compute the predictions, nor they were processed in batch.
\reftab{time_performance} shows the performance of the
models.

\phdtable[Time elapsed for each model to provide the output
predictions on the 94 files of the test
set]{time_performance}

By far, the fastest model is \gls{melisma}. This model is a
compiled program written in the \emph{C programming
language}. Thus, as a compiled program, it is intrinsically
faster than all other approaches, which are based on the
Python programming language. Out of the deep learning
models, the one in \textcite{micchi2021deep} seems to be the
fastest, followed by \gls{augmentednet} and
\textcite{chen2021attend}. The model in
\textcite{mcleod2021modular} is the slowest one to run on
raw \gls{musicxml} inputs, averaging nearly 2 minutes to
annotate each file. It is possible that one of the drawbacks
of the modular approach is that the computations are not run
in an end-to-end fashion as in \gls{mtl} approaches.
However, considering the time it takes for a human annotator
to annotate a long musical score (e.g., piano sonata), all
of these models are fairly good.
