% Copyright 2022 Néstor Nápoles López

The results in Figures \ref{fig:ablation_augmentation_loss}
and \ref{fig:ablation_augmentation_accuracy} show the
summary of the four experiments described above. All the
experiments were run for a fixed number of 200 epochs. The
lines shown correspond to the average validation loss
(\reffig{ablation_augmentation_loss}) and validation
accuracy (\reffig{ablation_augmentation_accuracy}) achieved
at epoch $n$ over all seven datasets. The validation
accuracy is slightly better in the experiment where both
data-augmentation techniques were included.
\reftab{augmentation_results} supports this reading of the
accuracy plot by showing average accuracy achieved in each
dataset by the end of the experiment.

\phdfigure[Validation loss of the different data
augmentation strategies]{ablation_augmentation_loss}

\phdfigure[Validation accuracy of the different data
 augmentation strategies]{ablation_augmentation_accuracy}

\phdtable[Average performance and standard deviation of the
four experiments with data augmentation over the seven
publicly available datasets]{augmentation_results}

Both data augmentation techniques have a positive effect in
the performance of the model compared to only using the
original data. The performance of the transposition
continues to be the most effective data-augmentation
technique, however, it also substantially increases the
training time compared to the synthesis itself. The most
effective technique is to use both, transposition and
synthetic examples, although the improvement is not
extensive. However, considering the difficulty of obtaining
expert-curated data for \gls{arna}, it seems to provide a
consistent improvement at the expense of more computation.

In future experiments, the proposed model is trained with
both data augmentation techniques simultaneously.
