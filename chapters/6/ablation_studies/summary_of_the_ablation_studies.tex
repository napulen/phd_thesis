% Copyright 2022 Néstor Nápoles López

\phdtablefith[Performance obtained in the ablation studies,
compared to the baseline configuration of the network. The
``Baseline'' row shows the average accuracy obtained in each
task across the 5-fold cross validation. Each row of the
ablation studies shows the difference in accuracy between
the value obtained in the experiment and the baseline. The
biggest drop in performance for each task column is
highlighted in bold font]{ablation}

\reftab{ablation} shows a summary of the performance of the
different configurations in the ablation experiments. Some
of these results confirm the expected results. For example,
removing the \gls{duration14} input representation affects
the \gls{harmonicrhythm7} more than any other change in the
model configuration. This might indicate that measure onsets
are important for a better chord segmentation. Removing the
\gls{chroma19} input representation has noticeable effects
on the prediction of chords (e.g., \gls{pcset121},
\gls{rn31}). Unexpectedly, it also affects the
\gls{soprano35} task more than any other \gls{satb35} task.
On the contrary, removing the \gls{bass19} input
representation affects the performance of the \gls{tenor35}
and \gls{alto35} tasks, in addition to the \gls{bass35},
which was the expected outcome.

Overall, as predicted, the biggest drop in performance
happens when the recurrent layers of the network are
removed. The recurrent layers have a notable effect on the
``key'' tasks: \gls{localkey38} and \gls{tonicization38}.
This supports the hypothesis that a longer musical context
is needed to estimate musical keys, in comparison to
estimating chords. It is interesting that among the chord
tasks, however, \gls{rn31} is the one affected the most by
the removal of the recurrent layers. This might be because
Roman numeral numerators are relative to the key, and thus
sensitive to the key context.

In the row of the baseline model, the standard deviation
($\sigma$) across the 5-fold cross validation is provided
for reference. All the differences in performance
highlighted in \reftab{ablation} are at least 2 standard
deviations below the performance of the baseline.
Furthermore, most modifications in the ablation studies have
a negative effect compared to the baseline model, which is
consistent with the observations during preliminary
experiments designing the network. One exception is the use
of a linear dense layer, which appears to be slightly above
the baseline in most tasks. In order to explore this
further, a subsequent experiment was performed between the
baseline and linear dense variations, adding data
augmentation. In that experiment, the advantage of the
linear dense variation faded away, with an average drop of
$-2.7\%$ across all tasks compared to the baseline. This
brings an important point about the ablation studies
presented here, which is that they do not provide
information about how the modifications scale with more (or
less) data. Thus, they provide hints about the contributions
of the different components of the network using a fixed
amount of data. The design of a neural network architecture
is importantly an empirical process, which requires
continuous experimentation in different scenarios. For this
reason, the baseline model presented in this section was
used in subsequent experiments, as it is the version of the
network that went through more experiments and datasets of
different sizes. Nevertheless, I still consider the ablation
studies interesting, as they permit to confront musical
intuitions with the components of the neural network. 
