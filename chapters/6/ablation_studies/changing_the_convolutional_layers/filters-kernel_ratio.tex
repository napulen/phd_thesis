% Copyright 2022 Néstor Nápoles López

One experiment is proposed, maintaining the number of
trainable parameters as close as possible to the baseline,
but deprioritizing short-term patterns. Instead, the number
of filters across all convolutional layers remains constant.

\phdparagraph{constant number of filters}
