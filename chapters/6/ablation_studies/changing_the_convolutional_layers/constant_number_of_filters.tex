% Copyright 2022 Néstor Nápoles López

In the baseline model, the number of filters decreases in
each layer, while the kernel size increases, as shown in
\reffig{ablation6}. An ablation experiment is proposed here,
where the number of filters across all convolutional layers
remains constant, keeping the number of trainable parameters
as close as possible to the baseline. A constant of $f=5$
was chosen for the number of filters in the ablation
experiment, as this results in a similar number of trainable
parameters as in the variable $f$ approach of the baseline.

Using this configuration, the network will learn the same
number of patterns at the level of \glspl{32nd} notes as it
does for \glspl{whole} notes. In the baseline configuration,
I hypothesize that more patterns in the short-term kernel
sizes (e.g., \glspl{32nd} notes) are beneficial to the
performance of the model. The modifications proposed are
shown in \reffig{ablation6}.

\phdfigure[Experiment with a constant number of filters in
each convolutional layer. The top figure is from the
baseline model, with a variable number of filters. The
bottom figure is the modified version. The affected sizes
are shown in bold typeface]{ablation6}
