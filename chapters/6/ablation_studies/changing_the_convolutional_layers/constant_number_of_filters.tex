% Copyright 2022 Néstor Nápoles López

In the baseline model, the number of filters decreases in
each layer, while the kernel size increases. An experiment
is proposed here, where the number of filters across all
convolutional layers remains constant, keeping the number of
trainable parameters as close as possible to the baseline.
Using this configuration, the network will learn a similar
number of patterns at the level of thirty-second notes
($\musThirtySecond$) as it does for whole notes
($\musWhole$). In the baseline configuration, I hypothesize
that more patterns in the short-term kernel sizes
($\musThirtySecond$) are beneficial to the performance of
the model.
