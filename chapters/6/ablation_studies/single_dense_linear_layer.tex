% Copyright 2022 Néstor Nápoles López

In the baseline model, two dense layers are positioned
between the convolutional and recurrent layers to reduce the
dimensionality of the representation after the convolutional
layers. The two dense layers have a \gls{relu} nonlinear
activation function. An experiment is proposed to explore
the effects of reducing the dimensionality without the
addition of nonlinearities. This is achived by replacing the
two \gls{relu}-activated dense layers with a single linear
layer without activation function. The latter exclusively
reduces the dimensionality of each timestep before the
recurrent layers. \reffig{ablation8} shows the changes
proposed in the experiment. 

\phdfigure[Replacement of two dense layers with a \gls{relu}
activation function for a single linear dense layer
]{ablation8}
