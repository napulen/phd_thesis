% Copyright 2022 Néstor Nápoles López

In the baseline model, two dense layers are positioned
between the convolutional and recurrent layers to reduce the
dimensionality of the representation after the convolutional
layers. The two dense layers have a \gls{relu} nonlinear
activation function. An experiment is proposed to explore
the effects of reducing the dimensionality without the
addition of nonlinearities. This is achieved by replacing
the two \gls{relu}-activated dense layers with a single
dense linear layer without activation function. The latter
exclusively projects the output of the convolutional layers
into a low-dimensionality representation before the
recurrent layers. \reffig{ablation8} shows the changes
proposed in the experiment. 

\phdfigure[Replacement of two nonlinear dense layers with a
single linear dense layer]{ablation8}
