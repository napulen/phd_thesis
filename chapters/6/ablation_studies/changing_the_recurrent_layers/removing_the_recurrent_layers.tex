% Copyright 2022 Néstor Nápoles López

The recurrent layers in the baseline model process the full
sequence of timesteps (i.e., 640 \gls{32nd}~notes). Except
for the convolutional layers 2--6 (shown in
\reffig{ablation_baseline_convolutional}), all other layers
in the network learn parameters at the level of an
individual timestep. Furthermore, the convolutional layer
with the longest time window (i.e., kernel size) learns
patterns across 32 timesteps (i.e., a \gls{whole} note). Any
musical patterns that occur beyond a \gls{whole} note
benefit from the recurrent layers. An ablation experiment is
proposed where the two recurrent layers in the baseline
model are removed. This should confirm the effects of losing
longer-term dependencies in the network. For example,
affecting the key estimation tasks: \gls{localkey38} and
\gls{tonicization38}. The modifications proposed are shown
in \reffig{ablation9}.

\phdfigure[An ablation experiment where the recurrent layers
have been removed entirely]{ablation9}
