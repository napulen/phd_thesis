% Copyright 2022 Néstor Nápoles López

The recurrent layers of the network are arguably the most
important trainable component of the \gls{crnn} proposed in
this dissertation. In the baseline model, two bidirectional
recurrent layers with \gls{gru} units are used.
\refsubsubsec{norecurrentlayers} explores the removal of
both recurrent layers and
\refsubsubsec{unidirectionalrecurrentlayers} explores the
removal of the bidirectional configuration. 
