% Copyright 2022 Néstor Nápoles López

The recurrent layers of the network are arguably the most
important trainable component of the \gls{crnn} proposed in
this dissertation. In the baseline model, two bidirectional
recurrent layers (\glspl{gru}) are used.
\refsubsubsec{removingtherecurrentlayers} explores the
removal of both recurrent layers and
\refsubsubsec{unidirectionalrecurrentlayers} explores the
effects of using a unidirectional configuration instead of
bidirectional. 
