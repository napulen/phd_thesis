% Copyright 2022 Néstor Nápoles López

The effects of two data-augmentation techniques were
explored: \emph{transposition} and \emph{synthesis}.
Transposition refers to the transposition of training
examples to a different key. Synthesis refers to a new
data-augmentation technique where artificial training
examples are generated from the \gls{rna} annotations. Both
data-augmentation techniques are described in
\refsec{dataaugmentation}. These techniques were applied on
the same baseline model described in the ablation studies
(see \refsubsec{baselinemodel}). In the experiments
performed, each of the seven individual datasets (see
\refsec{publiclyavailabledatasets}) was used to train the
baseline model four times, as shown in
\reftab{augmentation_experiments}. The first time, with only
the original data. The second time, with the original data
and additional data via \emph{transposition}. The third
time, with the original data and additional data via
\emph{synthesis}. The fourth time, with the original data
and additional data via \emph{transposition} and
\emph{synthesis}. As these two methods are not mutually
exclusive, the last experiment verifies whether they benefit
from each other. Figures
\ref{fig:ablation_augmentation_loss} and
\ref{fig:ablation_augmentation_accuracy} show the average
validation loss and average validation accuracy across the
seven datasets with each data-augmentation strategy.

\phdtable[Experiments performed on the baseline regarding
data augmentation]{augmentation_experiments}


