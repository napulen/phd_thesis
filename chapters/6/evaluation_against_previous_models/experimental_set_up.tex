% Copyright 2022 Néstor Nápoles López

The test used for the comparison was the same one used in
the experiment of \refsec{trainingontheaggregateddataset}.
There are a total of 94 files in the test set, which were
sampled from the seven publicly available datasets (see
\refsec{publiclyavailabledatasets}). All of the input files
in the test set were provided in the \gls{musicxml} format,
and their ground-truth annotations in \gls{romantext}. The
\gls{melisma} model does not support \gls{musicxml} inputs,
in this case, all the \gls{musicxml} files were translated
to \gls{humkern}, the format that \gls{melisma} supports.
Similarly, three models, \gls{melisma},
\textcite{chen2021attend, mcleod2021modular}, provided their
output annotations other formats. In these models, the
original outputs were translated to \gls{romantext}. All the
comparisons were performed between the ground-truth
\gls{romantext} in the test set, and each of the annotations
provided by the models (or the translations). 

Regarding the training of the model proposed in this
dissertation, it was trained for 200 epochs using a fixed
learning rate of $10^{-3}$ on the full training set plus
both methods of data augmentation. Afterwards, the model was
fine-tuned for an additional 100 epochs with a lower
learning rate of $10^{-5}$ on only the ``real'' training
data (i.e., removing transpositions and synthesized files).

