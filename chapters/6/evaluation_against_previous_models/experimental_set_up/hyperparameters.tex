% Copyright 2022 Néstor Nápoles López

The hyperparameters for training the model are summarized
here.

\phdparagraph{number of epochs}

The model was trained for 100 epochs using a fixed learning
rate of $10^{-3}$ on the full training set plus both methods
of data augmentation. Afterwards, the model was fine-tuned
for an additional 100 epochs with a lower learning rate of
$10^{-5}$.

\phdparagraph{optimizer}

The optimizer used was Adam without any learning rate
schedule.
