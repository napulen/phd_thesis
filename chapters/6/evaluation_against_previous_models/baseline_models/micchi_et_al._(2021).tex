% Copyright 2022 Néstor Nápoles López

The model introduced in \textcite{micchi2021deep} is an
extension of a previous model, \textcite{micchi2020not}. The
more recent model, with \gls{nade} layers was chosen for
comparison.

The published model was trained on several of the
collections introduced in
\refchap{dataacquisitionandpreparation}. A pretrained model
is not provided by the authors due to copyright reasons.
However, the implementation of all the necessary training
and inference code is provided. The outputs of the model are
written in the \gls{romantext} format, so no translation is
necessary. 

In the original publication \parencite{micchi2021deep}, the
model is trained with 80\% of the data, with 10\% of the
used for validation and 10\% for testing. The three splits
of data are randomly chosen. In the comparison done here,
the model was trained from scratch using 100\% of the
collection of scores in the original publication (all splits
together), minus the pieces that overlapped with the test
set of this comparison.
