% Copyright 2022 Néstor Nápoles López

The model introduced in \textcite{micchi2021deep} is an
extension of a previous model, \textcite{micchi2020not}. The
more recent model, with \gls{nade} layers was chosen for
comparison.

The published model was trained on several of the
collections introduced in
\refchap{dataacquisitionandpreparation}.\footnote{All of
them except \gls{kmt} and \gls{mps}.} A pretrained model is
unfortunately not provided by the authors due to copyright
reasons. However, the source code and dataset necessary to
train and run the model is provided. Thus, one was trained
from scratch. The outputs of the model are written in the
\gls{romantext} format, so no translation was necessary. 

In the original publication \parencite{micchi2021deep}, the
model was trained by randomly splitting the data between
training and validation. In the comparison done here, the
model was trained from scratch with 100\% of their dataset
used for training, minus the pieces that overlapped with the
test set used for this evaluation.
