% Copyright 2022 Néstor Nápoles López

The model introduced in \textcite{micchi2021deep} is an
extension of a previous model, \textcite{micchi2020not}. The
more recent model, with \gls{nade} layers was chosen for
comparison.

The published model was trained on several of the
collections introduced in
\refchap{dataacquisitionandpreparation}.\footnote{All of
them except \gls{kmt} and \gls{mps}.} A pretrained model is
unfortunately not provided by the authors due to copyright
reasons. However, the source code and dataset necessary to
train and run the model is provided. Thus, one was trained
from scratch. The outputs of the model are written in the
\gls{romantext} format, so no translation was necessary. 

In the original publication \parencite{micchi2021deep}, the
model was trained with 80\% of the data, leaving 20\% for
testing. The two splits of data are randomly generated. In
the comparison done here, the model was trained from scratch
with 100\% of their curated dataset, minus the pieces that
overlapped with the test set used for this evaluation.
