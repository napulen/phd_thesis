% Copyright 2022 Néstor Nápoles López

The model introduced in \textcite{micchi2021deep} is an
extension of a previous model, \textcite{micchi2020not}. The
more recent model, with \gls{nade} layers was chosen for
comparison.

The published model was trained on several of the
collections introduced in
\refchap{dataacquisitionandpreparation}.\footnote{All of
them except \gls{kmt} and \gls{mps}.} A pretrained model is
unfortunately not provided by the authors due to copyright
reasons. However, the source code and dataset necessary to
train and run the model was provided. Thus, it was easy to
train one from scratch. The outputs of the model are written
in the \gls{romantext} format, so no translation was
necessary. 

In the original publication \parencite{micchi2021deep}, this
model was trained by randomly splitting the dataset between
training and validation portions. In the comparison done
here, the model was trained from scratch with nearly all of
of their dataset used for training, except for 49 pieces
that overlapped with the test set used for this evaluation.
