% Copyright 2022 Néstor Nápoles López

\textcite{mcleod2021modular} proposed an \gls{arna} model
that differs in methodology to the ones presented in
\textcite{chen2021attend}, \textcite{micchi2021deep}, and
the approach presented here. For example, it substituted the
\gls{mtl} configuration with a modular approach based on
dedicated models per task. The published model was trained
twice with different datasets: a public dataset similar to
the one in \textcite{micchi2021deep}, and a larger private
dataset of their own. The authors provided source code and
pretrained models for both versions of the model. The output
annotations of the model are provided in a tabular \gls{tsv}
format, which comprises chord labels and key annotations.
Another output format based on the \gls{dcml} standard for
\gls{rna} annotations (see \refsubsubsec{theDCMLstandard})
was made available by the authors, but the tabular \gls{tsv}
format was used in these experiments. The pretrained model
used in this comparison was the one with the best results
according to the publication, using the private dataset.

Unfortunately, it does not seem that the \gls{natem}
algorithm is helping the \textcite{mcleod2021modular} model
in recognizing \gls{augsix} chords, as it only seems to
happe in 1\% of the cases for $\rnGer$ chords. It is
capable, however, of recognize \gls{neapolitan} chords.
