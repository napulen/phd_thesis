% Copyright 2022 Néstor Nápoles López

In a \gls{cnn} layer, the number of filters represents the
number of ``patterns'' that the network will learn from the
input. In the visual domain, these patterns could result in,
for example, edge-detection filters. In the musical domain,
the possible patterns are related with combinations of pitch
classes, pitch names, and note/measure onsets.

In this architecture, the number of filters is halved for
each consecutive 1D convolutional layer, as shown in
\refeq{number_of_filters}:

\begin{equation}
    \label{eq:number_of_filters}
    f_n = 2^{N - n}
\end{equation}

where $N$ is the number of 1D convolutional layers and $n$
is the n\textsuperscript{th} layer. The network is allowed
to store more filters (higher $f$) for short-term patterns
(low $k$) and fewer filters (lower $f$) for long-term
patterns (higher $k$). Preliminary experiments showed that
prioritizing short-term patterns in the \gls{cnn} layers
facilitated the network to learn certain features, such as
the ones related with chord segmentation. Moreover,
longer-term patterns benefitted key-finding tasks, but these
are mostly addressed by the \gls{rnn} layers of the network. 
