% Taken verbatim from AugmentedNet

\guide{Convolutional block}

\guide{Micchi and DenseNet-like convolutions.}
Using the feature maps of all previous layers as an input to
a convolutional layer has proven beneficial, for instance by
strengthening feature propagation and reducing the number of
parameters \cite{huang2017densely}. Moreover, DenseNet-like
architectures have shown to work well for the specific task
of functional harmony \cite{micchi2020not}.

\begin{figure*}
 \centerline{\includegraphics[width=\textwidth]{figs/network.png}}
 \caption{\emph{AugmentedNet}. The bass and chroma inputs
 are processed through independent convolutional blocks and
 then concatenated. Both convolutional blocks are identical
 and expanded on the top of the figure. A convolutional
 block has six 1D convolutional layers. Each layer doubles
 the kernel size (number of timesteps covered) and halves
 the number of output filters, prioritizing short-term
 dependencies but providing long-term context that benefits
 the subsequent GRU layers.}
 \label{fig:network}
\end{figure*}

\guide{Convolutional layers.}
We follow similar methods, reusing the feature maps computed
for a given timestep in subsequent convolutions. Figure
\ref{fig:network} provides a schematic diagram of our
network, with the convolutional block on the top left area
of the figure. In our preliminary experiments, we noticed
that different tonal tasks of functional harmony have
different time dependencies. For example, losing information
about a specific timestep often leads to poor performance in
predicting the inversion, whereas losing long-term
dependencies hinders performance in local key estimation.
Our architecture implicitly prioritizes short-term
dependencies in the initial convolutional layers, by having
more filters and covering less timesteps. Going further, the
convolutions provide more context about future timesteps,
but output a smaller number of filters. These increments (in
kernel size) and decrements (in filters) are done in powers
of 2. With a configuration of 6 convolutional layers in each
block (as shown in Figure \ref{fig:network}), the first
layer convolves a single timestep (`demisemiquaver') and the
sixth layer convolves 32 timesteps (`whole note'). The
output shape of the block is the original length of the
sequence with 82 features per timestep.
