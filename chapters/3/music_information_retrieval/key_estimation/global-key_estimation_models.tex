% Copyright 2021 Néstor Nápoles López

% This is \refsubsubsec{global-keyestimationmodels}, which
% introduces the global-key estimation models.

\phdparagraph{symbolic inputs}

% Longuet-Higgins (1971)
The \gls{gke} algorithm by
\textcite{longuethiggins1971interpreting}, arguably the
first one ever designed, computed the key of a piece through
an elimination process. The notes of the piece were read
from left to right, one by one, discarding all the keys
where the notes were not contained as diatonic steps of the
key, until a single key was remaining. This often lead to no
solutions, thus, the system contained several rules for
deciding the key in difficult cases. The algorithm
successfully explained the keys in Bach's
\emph{Well-Tempered Clavier}, however, it was relatively
easy to find adversarial examples where it did not work
\parencite{temperley2008pitchclass}.

% Krumhansl and Schmuckler (1990)

% The algorithm is based on a set of “key-profiles,” first
% proposed by Krumhansl and Kessler (1982), representing the
% stability or compatibility of each pitch-class relative to
% each key. The key-profiles are based on experiments in
% which participants were played a key-establishing musical
% context such as a cadence or scale, followed by a
% probe-tone, and were asked to judge how well the
% probe-tone “fit” given the context (on a scale of 1 to 7,
% with higher ratings representing better fitness).
% Krumhansl and Kessler averaged the rating across different
% contexts and keys to create a single major key-profile and
% minor key-profile, shown in Figure 2 (we will refer to
% these as the K-K profiles). The K-K key-profiles reflect
% some well accepted principles of Western tonality, such as
% the structural primacy of the tonic triad and of diatonic
% pitches over their chromatic embellishments. In both the
% major and minor profiles, the tonic pitch is rated most
% highly, followed by other notes of the tonic triad,
% followed by other notes of the scale (assuming the natural
% minor scale in minor), followed by chromatic notes. Given
% these key-profiles, the K-S algorithm judges the key of a
% piece by generating an “input vector”; this is, again, a
% twelve-valued vector, showing the total duration of each
% pitch-class in the piece. The correlation is then
% calculated between each key-profile vector and the input
% vector; the key whose profile yields the highest
% correlation value is the preferred key. The use of
% correlation means that a key will score higher if the
% peaks of its key-profile (such as the tonic-triad notes)
% have high values in the input vector. In other words, the
% listener’s sense of the fit between a pitch-class and a
% key (as reflected in the key-profiles) is assumed to be
% highly correlated with the frequency and duration of that
% pitch-class in pieces in that key. The K-S model has had
% great influence in the field of key-finding research. One
% question left open by the model is how to handle
% modulation: the model can output a key judgment for any
% segment of music it is given, but how is it to detect
% changes in key? Krumhansl herself (1990) proposed a simple
% variant of the model for this purpose, which outputs key
% judgments for each measure of a piece, based on the
% algorithm’s judgment for that measure (using the basic K-S
% algorithm) combined with lower-weighted judgments for the
% previous and following measures. Other ways of
% incorporating modulation into the K-S model have also been
% proposed (Huron & Parncutt, 1993; Schmuckler \& Tomovski,
% 2005; Shmulevich \& Yli-Harja, 2000; Temperley, 2001;
% Toiviainen \& Krumhansl, 2003). Other authors have
% presented models that differ from the K-S model in certain
% respects, but are still essentially distributional, in
% that they are affected only by the distribution of
% pitch-classes and not by the arrangement of notes in time.

Another algorithm, known as the \emph{Krumhansl-Schmuckler}
algorithm, was introduced in
\textcite{krumhansl1990cognitive}. Although introduced in
1990 as an automatic key finder, the main components of the
algorithm, the \emph{key profiles}, were introduced in 1982
\parencite{krumhansl1982tracing}. This algorithm and its
method based on key profiles influenced many other
algorithms in the symbolic and audio domains. It also
motivated the research of new key profiles using alternative
methods to the \emph{probe-tone} technique used by Krumhansl
and Kessler. The key was computed by correlating the pitch
histogram of the musical score with the pitch-class
distribution of the key profile. During a series of
experiments, \textcite{sapp2011computational} found that
when mistaken, this algorithm tends to predict the dominant
instead of the tonic.

% Vos and van Geenen (1996)
In 1996, a model for single-voiced pieces of music was
proposed by \textcite{vos1996parallelprocessing}. Similarly
to the \textcite{longuethiggins1971interpreting} model, it
was tested in excerpts of the \emph{Well-Tempered Clavier},
particularly, the themes of the fugues. Although the model
seemed to improve the results of the Longuet-Higgins model,
it did not become as influential as the Krumhansl-Schmuckler
in future research.

% Temperley (1999)
Responding to some concerns about the original
Krumhansl-Schmuckler algorithm,
\textcite{temperley1999whats} proposed two changes in a new
\gls{gke} algorithm. The first modification consisted in
replacing the \emph{Pearson correlation} function with a dot
product. The second modification was a change in the
probability distributions of the original Krumhansl-Kessler
key profiles. The new distribution was fine-tuned by
Temperley heuristically and through a trial-and-error
process. This algorithm provided better results than the
original algorithm and it was further extended into a
probabilistic framework using Bayes' rule in
\textcite{temperley2002bayesian}.

% Chew (2002)

% pitches are located in a three-dimensional space; every
% key is given a characteristic point in this space, and the
% key of a passage of music can then be identified by
% finding the average position of all events in the space
% and choosing the key whose “key point” is closest.

% proposes the Center of Effect Generator (CEG) key-finding
% method. In the CEG algorithm, a passage of music is mapped
% to a point within the three-dimensional space, known as
% the Center of Effect, by summing all of the pitches and
% determining a composite of their individual positions in
% the model. The algorithm then performs a nearest-neighbor
% search in order to locate the position of the key that is
% closest to the Center of Effect. The “closest key” can be
% interpreted as the global key for the piece, although the
% proximity can be measured to several keys, which allows
% for tonal ambiguities

Through the \emph{Spiral Array}, \textcite{chew2002spiral}
facilitated the modelling of tonality as a spatial
representation. In this representation, a sequence of notes
could be localized in a point of the tonal space, known as
the \emph{Center of Effect}, and related to a key through a
nearest-neighbours search. Although the geometric approach
for localizing a point in space influenced further research
on key-finding algorithms, the model did not perform better
than the approaches based on distributions and key profiles.

% Aarden (2003)
\textcite{aarden2003dynamic} proposed an alternative,
data-driven, methodology for obtaining a distribution of
scale degrees. This was used to generate a new set of key
profiles that could substitute the ones by
\textcite{krumhansl1982tracing} in a key-finding algorithm.
For this purpose, 1,000 songs from the Essen Folksong
Collection and 250 music excerpts from the MuseData database
were used for comparing the approaches. The distribution of
scale degrees outperformed the key profiles from
\textcite{krumhansl1982tracing}.

% These results were confirmed by other researchers
% \parencite{albrecht2013use}. In future experiments, Sapp
% \parencite{sapp2011computational} showed that, when unable
% to predict the correct key, the Aarden-Essen distribution
% tends to favor the subdominant instead of the tonic.

% % Yoshino and Abe (2005)

% similar to Vos and Van Geenen’s, in that pitches
% contribute points to keys depending on their function
% within the key; temporal ordering is not considered,
% except to distinguish “ornamental” chromatic tones from
% other chromatic tones

% Bellmann (2005)
A similar effort for scale-degree probability distributions
was proposed by \textcite{bellmann2006about}. Bellman used
the chord frequencies collected in the PhD \thesisdiss{} by
\textcite{budge1943study} to create a new scale degree
probability distribution. This probability distribution was
used to obtain the most likely key of a fragment of a score
(typically, a measure long) by computing the dot product
between the notes found in the excerpt and the scale degree
distribution. The results of this algorithm were also
favourable, showing that chord frequencies can be helpful in
estimating the musical key.

% % Temperley (2007) Distributional keyfinding model based
% on probabilistic reasoning. This probabilistic model
% assumes a generative model in which melodies are generated
% from keys. A key-profile in this case represents a
% probability function, indicating the probability of each
% scale-degree given a key.


% % Madsen and Widmer (2007) argue that in addition to the
% pitch-class distribution, the order of notes appearing in
% a piece of music may also help determine the key. They
% propose a key-finding system that incorporates this
% temporal information by also analyzing the distribution of
% intervals within a piece of music. Interval Profiles are
% 12x12 matrices representing the transition probability
% between any two scale degrees. The profiles are then
% learned from key annotated data for all 24 keys. Using a
% corpus of 8325 Finnish folk songs in \gls{midi} format,
% the system was trained using 5550 songs and evaluated with
% the remainder. A comparison was also performed between the
% use of Interval Profiles and several types of pitch class
% profiles. The maximum key recognition rate using the
% Interval Profiles was 80.2\%, whereas the maximum
% recognition rate using pitch class profiles was 71\%.

% % Gatzsche et al. (2007)

% % Quinn (2010)

% Sapp (2011)
In order to explore the capabilities of different key
profiles at different window lengths,
\textcite{sapp2011computational} performed a series of
experiments running every possible window in a piece with
different key profiles. The results of such computations
were visualized and rendered into what Sapp referred to as
\emph{keyscapes} \parencite{sapp2001harmonic}. The algorithm
used for computing the keys was the original
Krumhansl-Schmuckler correlation algorithm, alternating the
different key profiles. Additionally, Sapp introduced a new
key profile named \emph{simple weights}.

% Albrecht and Shanahan (2013)
\textcite{albrecht2013use} introduced a new key-finding
algorithm that improves the accuracy for pieces in the minor
mode. This algorithm utilizes Euclidean distance to measure
the similarity between the notes in the piece and a new key
profile. The key profile was obtained by analyzing the first
and last measures of a training dataset, which consisted of
982 pieces of music from the baroque to the romantic period.
The model outperformed every other symbolic key-finding
algorithm when used in this dataset.

% % Lam and Lee (2014)

% % White (2015)

% % White (2018)

% N\'apoles L\'opez et al. (2019)
In 2019, we introduced a new key-finding algorithm that
works in the symbolic and audio domains
\parencite{napoleslopez2019keyfinding}. This algorithm was
able to operate as both a \gls{gke} and \gls{lke} model, and
it is based on an \gls{hmm}. The algorithm provided
state-of-the-art performance in the symbolic domain,
however, it underperformed for pieces in minor modes, unlike
the \textcite{albrecht2013use} algorithm, which performs
well in the minor mode.

% TODO: Are these all the global-key-finding algorithms for
% symbolic?

\phdparagraph{audio inputs}

% Leman (1992) derives key directly from an acoustic signal,
% rather than from a representation where notes have already
% been identified. The model is essentially a key-profile
% model, but in this case the input vector represents the
% strength of each pitch-class (and its harmonics) in the
% auditory signal; key-profiles are generated in a similar
% fashion, based on the frequency content of the primary
% chords of each key.

% the system is based heavily on a model of the human
% auditory system and consists of two stages. The first step
% is to extract local tone centers in a bottom-up manner for
% the piece of music. The second stage of the system uses a
% pattern-matching algorithm to compare the extracted tone
% center data with predetermined templates derived from
% self-organizing maps

Arguably the first audio \gls{gke} algorithm was proposed by
\textcite{leman1992een}. It consisted of two stages. In the
first stage, the algorithm extracted the strength of each
pitch-class, including its harmonics, from the audio signal.
In the second stage, the algorithm tried to match the
pitch-class information of the first stage with a set of
templates, analogous to a key profile, which were obtained
through \acrfull{som}.

% Izmirli and Bilgen (1994) proposed a system for audio key
% finding that implements partial score transcription in
% combination with a pattern-matching algorithm. In the
% first stage of the system, the fast Fourier transform
% (FFT) function is used in order to convert a single-part,
% melodic audio input into a sequence of note intervals with
% associated onset times. A second stage then employs a
% finite-state automata algorithm to compare the note
% sequences with predetermined scale patterns. The model
% then outputs a tonal context vector, where each element is
% known as a tonal component. Each tonal component
% represents the extent of any given scale usage within the
% melody for the corresponding location in time.

\textcite{izmirli1994recognition} proposed a model that also
consisted of two stages. The first stage converted
monophonic audio signals into a sequence of note intervals
and their occurrence times, similar to a \emph{piano-roll}
representation. The second stage used a series of finite
state automatas to match patterns of scales. In total, they
used three pattern-matching automatas: major scale, natural
minor scale, and harmonic minor scale.

% Purwins et al. (2000) the system employs the CQtransform
% to extract a pitch-class distribution from the audio
% signal. A fuzzy distance algorithm is then used to compare
% the pitch-class distribution with the cognitive-based
% templates. The system is able to track the key over time
% and thus is capable of identifying modulations in the
% music. An evaluation was performed using Chopin’s C minor
% prelude, Op. 28, No. 20 and was fairly successful at
% tracking the key, although no quantitative results were
% explicitly reported.

\textcite{purwins2000new} proposed a model that was based on
the constant-Q transform, extracting pitch-class
distributions from the audio signal based on a few basic
music-theoretical assumptions (i.e., octave equivalence and
the division of the octave in the chromatic scale). Using
these distributions, the system tracked the key over time by
comparing the distributions of the signals with the
\textcite{krumhansl1982tracing} key profiles. The model was
not quantitatively evaluated but an example of the keys
tracked in Chopin's Prelude Op.~28 No.~20 was provided,
showing similar key segmentations as the ones provided by an
expert annotator.


% Gomez and Herrera (2004) noted that the majority of audio
% key detection models developed up until 2004 were based on
% perceptual studies of tonality, which they called
% cognition-inspired models. They performed an experiment in
% which they directly compared an implementation of a
% cognition-inspired model with several machine-learning
% algorithms for audio key determination. The
% cognition-inspired model was based on the K-S algorithm
% but extended to handle polyphonic audio input. Numerous
% machine learning techniques were implemented, including
% binary trees, Bayesian estimation, neural networks, and
% support vector machines. The various algorithms were
% evaluated on three criteria: estimating the “key note”
% (i.e., tonic), the mode, and the “tonality” (i.e., tonic
% and mode). A corpus of 878 excerpts of classical music
% from various composers was used for training and testing.
% The excerpts were split into two sets: 661 excerpts for
% training and 217 excerpts for evaluation. The results,
% summarized in Figure 2.9, show that for the case of
% estimating the “tonality,” the best machine learning
% algorithm (a multilayer perceptron, neural network)
% outperforms the cognition-inspired model, but a
% combination of the two approaches produces the best
% results.

\textcite{gomez2004estimating} proposed a comparison of what
they named ``cognition-inspired'' models, against models
derived from machine learning techniques. The
cognition-inspired model was a modification of the
Krumhansl-Schmuckler algorithm that worked with
\acrfull{hpcp} features as input, instead of note
histograms. The machine learning models tested included
binary trees, Bayesian estimation, \glspl{ann}, \glspl{svm},
boosting, and bagging. In an experiment with 661 audio files
used for training and 217 used for testing, the best machine
learning model had a slightly better performance than the
cognitive-based model, however, the best overall performance
resulted from the combination of both models.

% % Pauws (2004) The system incorporates signal processing
% techniques designed to improve the salience of the
% extracted pitch-class distribution. The pitch-class
% distribution is then used as input to the maximum-key
% profile algorithm in order to identify the key. The model
% was tested on a corpus of 237 classical piano sonatas,
% with a maximum key identification rate of 66.2\%.

% % Shenoy et al. (2004) present a novel, rule-based
% approach for estimating the key of an audio signal. The
% system utilizes a combination of pitch-class distribution
% information, rhythmic information, and chord progression
% patterns in order to estimate the key. The audio signal is
% first segmented into quarter note frames using onset
% detection and dynamic programming techniques. Once
% segmented, an algorithm is employed to extract the
% pitchclass distribution for each frame. Using this
% information, the system is then able to make inferences
% about the presence of chords over the duration of the
% audio signal. Finally, the chord progression patterns are
% used to make an estimate for the key of the piece. The
% system was evaluated with 20 popular English songs and had
% a key recognition rate of 90\%.

% % Martens et al. (2004) implemented a model using a
% classification-tree for key recognition. The
% classification tree was trained using 264 pitch-class
% templates that were constructed from Shepard sequences and
% chord sequences of various synthesized instruments. They
% conducted an experiment that compared the performance of
% the tree-based system with a classical distance-based
% model using two pieces: “Eternally” by Quadran and
% “Inventions No. 1 in C major” by J. S. Bach. The results
% led them to favor the classification-tree system due to
% its ability to stabilize key estimations over longer time
% periods. They also noted the advantage of being able to
% tune the system for specific types of music by using a
% corresponding category of music to train the model

% Burgoyne and Saul (2005)
\textcite{burgoyne2005learning} provided a model for
harmonic analysis in the audio domain, using a corpus of
Mozart symphonies (15 movements) for training. The input for
their model was a chromagram vector (denoted as pitch-class
profile by the authors). They used an \acrshort{hmm} to
train Dirichlet distributions for major and minor keys on
the chromagram vectors. Their system attempted to find
chords and keys simultaneously. The model was tested on a
single piece, the second movement of Mozart's KV550.

% Chai and Vercoe (2005)
\textcite{chai2005detection} proposed an \acrshort{hmm} to
detect keys in audio files. Their approach took as its input
a chromagram vector of 24 bins, extracting the best match of
a major-and-relative-minor pair of keys (this could be
thought as an algorithm that extracted the key signature). A
second step used heuristics to determine the mode. In order
to evaluate their algorithm, they used 10 pieces of piano
music from the classical and romantic period, which were
manually annotated by the authors.

% Chuan and Chew (2005) they formulate hypotheses for
% sources of errors during the pitch class generation stage
% and propose a modified algorithm that uses fuzzy analysis
% in order to eliminate some of the errors. The fuzzy
% analysis method consists of three main components:
% clarifying low frequencies, adaptive level weighting, and
% flattening high and low values. They performed a direct
% comparison of the fuzzy analysis key-finding system with
% two other models: a peak detection model and a \gls{midi}
% key-finding model. The evaluation utilized excerpts from a
% corpus of 410 classical music \gls{midi} files, where only
% the first 15 seconds of the first movement was considered.
% The fuzzy analysis and peak detection algorithms operated
% on audio files that were synthesized using Winamp, and the
% \gls{midi} key-finding model operated directly on the
% \gls{midi} files. The maximum key identification rates for
% the peak detection, fuzzy analysis, and \gls{midi}
% key-finding models were 70.17\%, 75.25\%, and 80.34\%,
% respectively.

% we evaluate the technique by comparing the results of
% symbolic (\gls{midi}) key finding, audio key finding with
% peak detection, and audio key finding with fuzzy analysis.
% We showed that the fuzzy analysis technique was superior
% to a simple peak detection policy, increasing the
% percentage of correct key identifications by 12.18\% on
% average.

\textcite{chuan2005polyphonic} proposed a real-time
algorithm based on the extraction of pitches and
pitch-strength through the \acrfull{fft}. The \gls{gke}
model was based on the \emph{Center of Effect Generator}
algorithm by \textcite{chew2002spiral} and determined the
key based on the pitch-strength information. In their
evaluation, the model achieved a recognition rate of 96\%
within the first fifteen seconds of their test set, which
consisted of 61 audio files with different performances of
21 Mozart Symphonies. Their system outperformed the
Krumhansl-Schmuckler algorithm
\parencite{krumhansl1990cognitive} and the modified version
by \textcite{temperley1999whats} when evaluated within the
first seconds of the recordings. A further improvement was
proposed in \textcite{chuan2005fuzzy}, which incorporated
fuzzy analysis to improve the pitch-class distributions
computed from the audio. This model was evaluated during the
first \emph{Audio Key-Finding} \gls{mirex} evaluation, in
2005. However, the model was outperformed by other
approaches of the evaluation
campaign.\footnotelink{https://www.music-ir.org/mirex/wiki/2005:Audio\_Key\_Finding\_Results}

% % Zhu et al. (2005) utilizes the CQ-transform and detects
% the key in two distinct steps: diatonic scale root
% estimation and mode determination. The system is evaluated
% on a corpus of 60 pop songs and 12 classical music
% recordings, using only the first 2 minutes of each piece.
% The correct scale root was detected for 91\% of the pop
% songs but only 50\% of the classical music pieces. The
% rate of successful mode determination for the pop songs
% was 90\% and 83.3\% for the classical pieces

% Harte et al. (2006) they then propose an audio key-finding
% model that is based on projecting collections of pitches
% onto the interior space contained by the hypertorus. This
% is essentially mapping to three distinct feature spaces:
% the circle of fifths, the circle of major thirds, and the
% circle of minor thirds. This 6-dimensional space is called
% the tonal centroid. The algorithm first applies the
% CQ-transform in order to extract the pitch-class
% distribution. The 12-D pitch-class distribution is then
% mapped to the 6-D tonal centroid with a mapping matrix.
% The algorithm was applied in a chord recognition system,
% however, the authors point out that it could be adapted
% for other classification tasks such as key detection. Fig.
% 2.12: If enharmonic and octave equivalence are considered,
% then the Spiral Array model can be represented as a
% hypertorus (from Harte et al. 2006).

\textcite{harte2006detecting} proposed a new model that
applied a similar principle to the spatial representation of
\textcite{chew2000towards}. It had a 6-dimensional interior
space contained by the surface of a hypertorus. The 6
dimensions of their model correspond to 2-axes for
localizing a point in the circle of fifths, 2-axes for
localizing a point in the circle of major thirds, and 2-axes
for localizing a point in the circle of minor thirds. Using
this spatial representation, a similar methodology to Chew's
\emph{Center of Effect} \parencite{chew2002spiral} was used
for computing the key. The authors presented an evaluation
of the model in the detection of chord changes within 16
songs of \emph{The Beatles}.

% TODO: Why chord changes if its a GKE algorithm?

% % Izmirli and Bilgen (2006)

% % Mardirossian and Chew (2006)

% Noland and Sandler (2006)
\textcite{noland2006key} proposed a model based on an
\gls{hmm}. The model comprised 24 hidden states, which
represented each of the major and minor keys. As
observations, the \gls{hmm} considered a chord transition (a
pair of consecutive chords). The emission probabilities of
the model were initialized by assuming a strong correlation
between the key and the key implied in a given chord
transition. The transition probabilities of the model were
obtained using a table of correlations between the
transposed \textcite{krumhansl1982tracing} key profiles. The
model was evaluated in a dataset of 110 songs of the
Beatles, for which the chord transitions were already
annotated. The algorithm was able to predict the key of the
songs in 91\% of the cases. A further change on the
observations of the \gls{hmm} was necessary to work directly
on audio inputs.

% Peeters (2006) implements one \gls{hmm} for each of the 24
% possible keys. A front-end algorithm is used to extract a
% sequence of time-based chroma-vectors (i.e., pitch-class
% distributions) for each of the songs in a training set of
% key annotated music. All of the chroma-vectors for songs
% in the major mode are then mapped to C major and all of
% the chroma-vectors for songs in the minor mode are mapped
% to C minor.

\textcite{peeters2006chromabased} proposed a model that
extracted information about the periodicity of pitches from
the audio signals, mapped that information into the chroma
domain, and decided the global key of the music piece from a
succession of chroma-vectors over time. For the initial
analysis, Peeters proposed a Harmonic Peak Substraction
algorithm, which reduced the influence of the higher
harmonics of each pitch. The pre-processed signal was mapped
to the chroma domain and used as the observation for an
\gls{hmm} model that predicted the global key. The \gls{hmm}
classification consisted of one \gls{hmm} for each of the 24
possible keys, which were trained using the Baum-Welch
algorithm.\footnote{See \textcite{rabiner1989tutorial} for
an introduction to the Baum-Welch algorithm in the context
of \glspl{hmm}.} The model was tested in 302 audio files
containing classical piano, chamber, and orchestral music,
obtaining a 89.1\% accuracy using the \gls{mirex} weighted
evaluation score, shown in \reftab{mirex_score}.

\phdtable[The weighted evaluation score for key predictions
proposed by \gls{mirex}]{mirex_score}

% TODO: Which year? The \gls{mirex} evaluation

% % van der Par et al. (2006) present an extension to the
% work of Pauws (2004) in which they utilize three different
% temporal weighting functions in the calculation of the
% pitchclass templates. This results in three different
% templates for each key. Similarly, during the actual key
% detection, three different pitch-class profiles are
% extracted, one for each temporal weighting function. Each
% of the three pitch-class profiles is then correlated with
% the corresponding templates and a final correlation value
% is calculated from the combined values. The system was
% evaluated using the same corpus of 237 classical piano
% sonatas as Pauws (2004) and received a maximum key
% recognition rate of 98.1%.


% Catteau et al. (2007)
\textcite{catteau2007probabilistic} devised a model that
simultaneously predicts chords and keys in audio signals.
Their model is an extension of a previous model by
\textcite{bello2005robust}, and it is based on music theory
domain knowledge, requiring no training. The music theory
components of the model were inspired by Lerdahl's Tonal
Pitch Space \parencite{lerdahl2005tonal}. They tested their
model with synthesized audio, recorded audio, and presented
their model for evaluation in the Key and Chord detection
tasks from \gls{mirex}. 
% For testing the local keys, they used short sequences of
% chords, similar as those from
% \textcite{krumhansl1982tracing}. Their performance on
% synthesized audio was good, but the performance on
% recorded audio decreased significantly.

% TODO: Which year, \gls{mirex}

% Izmirli (2007) a model for localized key finding from
% audio is proposed. Besides being able to estimate the key
% in which a piece starts, the model can also identify
% points of modulation and label multiple sections with
% their key names throughout a single piece.

% Lee and Slaney (2007)
\textcite{lee2007unified} proposed a model for chord and key
estimation from audio sources. The model was trained by
creating a separate \gls{hmm} for each of the 24 keys. In
order to obtain the training data, they annotated several
\gls{midi} files with harmonic analysis labels. These
harmonic analyses were computed automatically using the
Melisma Music Analyzer \parencite{temperley2004cognition}.
The harmonic analysis corpus was obtained from
\texttt{mididb.com}, and consisted of 1046 \gls{midi} files
of Rock music. The annotated files were synthesized with a
sample-based synthesizer (Timidity++, using the FluidR3
soundfont) and perfectly aligned to the annotations. Their
chord detection model was able to distinguish only major and
minor triads.

% % Cheng et al. (2008)

% % Lee and Slaney (2008)

% % Arndt et al. (2008) making use of a model based on
% circular pitch spaces (CPS). They introduce the music
% theory-based concept of a CPS and go on to present a
% geometric tonality model that describes the relationship
% between keys. Furthermore, they implement an audio
% key-finding system that makes use of the model. The
% CQ-transform is employed in order to extract a pitch-class
% distribution, which is in turn input to the CPS model. The
% model then maps the vector to 7 different circular pitch
% spaces, which essentially gives 7 different predictions
% for the key.

% % Hu and Saul (2009)

% Campbell (2010)
\textcite{campbell2010automatic} proposed an algorithm based
on four stages: frequency analysis, pitch-class extraction,
pitch-class aggregation, and key classification. A thorough
experiment was performed with different solutions for each
of the stages. The best performing model was evaluated in a
dataset of classical music, another one of popular music,
and an audio-synthesized dataset of \gls{midi} files of
classical music. The best performing model consisted of
features extracted with the \emph{jAudio} feature extractor
\parencite{mcennis2005jaudio} and a \gls{knn} classifier.

% Mauch and Dixon (2010)
\textcite{mauch2010simultaneous} proposed a model that
predicted chords, bass notes, metric position of the chords,
and the musical key. Although it is claimed that the model
detects the key of the audio input, the model in fact
computes the key signature, classifying a major key and its
relative minor with the same class. Due to this restriction,
the model was not explicitly evaluated on musical key.
Nevertheless, using the key classifications of the model as
features, it showed to improve the results of their main
task, chord recognition.

% % Rocher et al. (2010)

% % Papadopoulos and Tzanetakis (2012)

% % Dieleman and Schrauwen (2014)

% % McVicar et al. (2014)

% % Pauwels and Martens (2014)

% % Faraldo et al. (2016)

% % Bernardes et al. (2017)

% % Faraldo et al. (2017)

% % Dawson and Zielinski (2018)

% % Korzeniowski (2018)

% Korzeniowski and Widmer (2018) we show the network only
% short snippets instead of the whole piece at training
% time. [...] From our datasets, we found 20 seconds to be
% sufficient (with the exception of classical music, which
% we need to treat differently, due to the possibility of
% extended periods of modulation). Each time the network is
% presented a song, we cut a random 20 s snippet from the
% spectrogram. The network thus sees a different variation
% of each song every epoch.

% we expect this modification to have the following effects.
% (i) Back-propagation will be faster and require less
% memory, because the network sees shorter snippets; we can
% thus train faster, and process larger models. (ii) The
% network will be less prone to over-fitting, since it
% almost never sees the same training input; we expect the
% model to generalise better. (iii) The network will be
% forced to find evidence for a key in each excerpt of the
% training pieces, instead of relying on parts where the key
% is more obvious; by asking more of the model, we expect it
% to pick up more subtle relationships between the audio and
% its key.

\textcite{korzeniowski2018genreagnostic} proposed a
key-finding model that generalized across different genres
of music. The model was based on a \gls{cnn} that received a
log-magnitude log-frequency spectrogram as its input.
Initially, during training, the model received the full
spectrogram, however, this computation turned out to be
expensive. Korzeniowski and Widmer proposed showing the
network only \emph{snippets} (20 seconds) of the
spectrogram. This improvement decreased the training time
but assumed that any arbitrary 20-second audio fragment
would display the global key, disallowing the model to
operate with music files featuring modulations. The model
was trained on three datasets: the GiantSteps \gls{mtg} Key
dataset by \textcite{faraldo2016key}, the McGill Billboard
dataset by \textcite{burgoyne2011expert}, and an internal
dataset with classical music. The model was submitted to the
\gls{mirex} campaign of 2018 and it was successful at
generalizing the key of different genres, outperforming
existing audio \gls{gke} models.

% % Schreiber and Mueller (2019)

% N\'apoles L\'opez et al. (2019)
We proposed a key-finding model in 2019
\parencite{napoleslopez2019keyfinding}. This model performed
both \gls{gke} and \gls{lke}, and it was able to operate in
the symbolic and audio domains. The model was originally
designed for symbolic inputs, however, it was able to work
in the audio domain by making use of the chroma features by
\textcite{mauch2010approximate} and turning them into
discrete values using a threshold. The model was submitted
to the \gls{mirex} Audio Key Estimation in 2018 and 2019. In
2018, the model was submitted with a single key profile,
which provided the best generalization in our experiments.
In 2019, the model was improved by adding a meta-classifier
which predicted the global key based on the predictions of
several key profiles. The results of the
2018\footnotelink{https://www.music-ir.org/mirex/wiki/2018:Audio\_Key\_Detection\_Results}
and
2019\footnotelink{https://www.music-ir.org/mirex/wiki/2019:Audio\_Key\_Detection\_Results}
models are available on the \gls{mirex} results website.

% Conclusion
After the pioneering models, many different alternatives
have been proposed for finding the musical key of symbolic
and audio inputs. The models consider different
methodologies, initially inspired by music perception and
cognition and slowly transitioning into data-driven and
machine learning approaches. As may be observed from the
evaluation sections of the literature, it is difficult to
know which models are better than the rest, mostly due to
little agreement (except for the \gls{mirex} campaigns) in
the methods used for comparing the capabilities of one model
or another, or in the use of a similar test dataset.
Furthermore, some of the models output different results,
ranging from key signatures, local keys, global keys, and
chords, which further complicates the comparison of
key-finding models. Nevertheless, as a general trend, it can
be seen that the research by \textcite{krumhansl1982tracing}
that introduced the key profiles, together with the use of
chroma features, have been very useful in the design of
newer, more robust \gls{gke} models throughout the years.
These techniques are also relevant for \gls{lke} models,
which are described next.
