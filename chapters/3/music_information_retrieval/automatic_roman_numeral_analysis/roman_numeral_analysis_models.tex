% Copyright 2021 Néstor Nápoles López

% This is \refsubsubsec{romannumeralanalysismodels}, which
% introduces the roman numeral analysis models.

\guide{First end-to-end system.}
The first end-to-end system of automatic Roman numeral
analysis can be attributed to the independent contributions
of Temperley and Sleator \parencite{temperley2004cognition},
and Sapp \parencite{sapp2009tsroot}. Temperley's preference
rules were implemented by Sleator as part of a suite of
programs called \emph{Melisma (version
1)}.\footnote{Although Melisma (version 2) exists, it does
not provide Roman numeral analysis output.} The
contributions of Sapp to the Humdrum toolkit
\parencite{huron2002music} made it possible to process any
encoded musical score with Roman numeral annotations using
\emph{Melisma}. A more thorough review of the Melisma system
can be found in \textcite{napoleslopez2017automatic}.

\guide{Before deep learning.}
Notable subsequent studies include
\textcite{raphael2004functional},
\textcite{illescas2007harmonic}, and
\textcite{magalhaes2011functional}, who proposed \glspl{hmm}, dynamic programming, and grammar-based
approaches, respectively.

\guide{Chen and Su (2018).}
More recently, deep neural networks have become the
preferred tool for approaching this problem.
\textcite{chen2018functional} were the first to introduce
`multi-task learning' (MTL) \parencite{ruder2017overview} to
the problem as a suitable way for the neural network to
share representations between related tonal tasks. Chen and
Su's model consists of a bidirectional \gls{lstm}
\parencite{hochreiter1997long} followed by task-specific
dense layers, which implement the MTL aspect. In this work,
the authors also introduced the `Beethoven Piano Sonata
Functional Harmony' dataset for evaluating such models. The
MTL layout outperformed single-task configurations and it
has continued to prove the best-performing approach in
subsequent deep learning studies. In subsequent work, the
same authors have adopted Transformer-based networks to deal
with functional harmony and \gls{acr}
\parencite{chen2019harmony, chen2021attend}. The work with
these networks has explored the capability of the attention
mechanisms to improve the performance of \gls{acr}, paying
special consideration to chord segmentation and its
evaluation.

\guide{Micchi et al (2020).}
\textcite{micchi2020not}, in turn, proposed a DenseNet-like
\parencite{huang2017densely} convolutional neural network,
followed by a recurrent component. The recurrent component
consists of a bidirectional \gls{gru}
\parencite{cho2014learning} connected to task-specific dense
layers, similar to those of \textcite{chen2018functional}.
In their experiments, the DenseNet-like convolutions
outperformed dilated convolutions and a \gls{gru} by itself
(i.e., with pooling instead of the convolutional blocks).
Micchi et al.~also demonstrated the positive effect of using
pitch \textit{spelling} in the inputs and outputs. This
confers at least two advantages: it provides a more
informative output (e.g., not only the correct key, but the
correct spelling between two enharmonic keys), and it
increases the possible number of transpositions available
for data augmentation.
