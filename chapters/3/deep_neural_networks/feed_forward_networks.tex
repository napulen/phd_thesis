% Copyright 2021 Néstor Nápoles López

% This is \refsubsec{feedforwardnetworks}, which introduces
% the feed forward networks.

An \gls{ann} is a machine learning algorithm that models
arbitrary functions by automatically learning \emph{weights}
(also known as \emph{parameters}) connecting the different
nodes of the neural network. Generally, a non-linear
activation function is applied to such weights, introducing
a nonlinear behavior in the neural network that allows it to
learn functions of higher complexity, which a linear model
could not possibly learn. The learning of the weights is not
achieved by programming task-specific rules but instead, by
identifying simple characteristics of the training examples
and extending them into more complex, more abstract
characteristics through the \emph{backpropagation}
algorithm. The process of decomposing an example into a
combination of simpler features is known as
\emph{representation learning}, and it is one of the main
ideas that differentiate an \gls{ann} from other classes of
machine learning.

The study of \glspl{ann} started around the 1940s, and it
has been known through different names throughout the years
\parencite{goodfellow2016deep}.

The research on \glspl{ann} can be traced back to the 1940s,
when a bio-inspired \emph{neuron} model was introduced
\parencite{mcculloch1943logical}. This neuron allowed to
model very simple functions by manually setting the weights
that connected the input into the neuron. This idea was
later extended to propose the Perceptron
\parencite{rosenblatt1958perceptron} and Adaline
\parencite{widrow1960adaptive} models, which were able to
automatically derivate such weights from the data. Although
these models showed promise, their popularity decreased
significantly when it was demonstrated that they could not
learn relatively simple functions, like the \emph{XOR}
function \parencite{minsky1972perceptrons}. This wave of
research (1940--1960) is often referred as the
\emph{cybernetics} wave of neural networks research
\parencite{goodfellow2016deep}.

Following the wave of cybernetics, a new one started around
the 1980s-1990s, colloquially known as \emph{connectionism}.
During the work of the \emph{connectionists},\footnote{A
term typically used to refer to the scientists of this time
period and research field} the research community benefited
from the development of the current form of the
back-propagation algorithm
\parencite{rumelhart1988learning}. The back-propagation
algorithm became (and remains) an elemental process in the
training of neural networks, which allows to propagate the
error throughout the network by making use of the
\emph{chain rule}. Finding the derivatives of each parameter
in the network, the values of such parameters can be updated
in the ``right direction'' (against the gradient) to improve
the classification accuracy through the next batch of
training examples. This facilitates the automatic training
of large and complicated neural networks, with multiple
layers, neurons, and non-linear activation functions. Even
though this and other improvements made neural networks a
promising area of research, they were still very difficult
to train in practice (due to the difficulty of finding a
good initialization of the weights) and were typically
outperformed by domain-knowledge techniques, losing the
interest of many scientists as a consequence.

Finally, a third wave of research started around 2006, when
new methods for training neural networks were introduced
\parencite{hinton2006fast}. These new methods not only
facilitated the training of neural networks but the training
of \textbf{much larger} neural networks. The interest in
such larger architectures extended, and in a historical
evaluation of the ImageNet dataset in 2012
\parencite{krizhevsky2012imagenet}, neural networks
outperformed the most sophisticated methods of computer
vision, setting a prominent gap in performance between
neural networks and every other method. This had an enormous
implication in the way that neural networks were perceived
by the research community and motivated their application
into different problems and fields of study. We know this
last wave of research as \emph{deep learning}, and it is
currently an active and growing wave of research across many
fields. Around this umbrella term of \emph{deep learning},
many state-of-the-art machine learning techniques have been
developed and continue to be improved.

\guide{Deep learning and \gls{mir}} After the growing
interest for neural networks in the wave of deep learning
research, many new models, architectures, and applications
have been proposed and put into practice in recent years.
%achieving good results and generating subfields of research
%within the umbrella term of deep learning.
Among the most important innovations to the original neural
network architectures, we can consider \glspl{cnn},
\glspl{rnn}, and Transformer networks.

\guide{Other deep learning architectures in \gls{mir}}
Throughout the years, the solutions presented in the
research field of deep learning have been applied to
multiple \gls{mir} tasks. The most popular ones have already
been discussed, however, other applications include the
historical \gls{som} in the early 2000s
\parencite{kiernan2000scorebased, harford2003automatic},
\glspl{dbn} \parencite{hamel2010learning,
schmidt2013learning, chacon2014developing,
raczynski2010multiple, battenberg2012analyzing,
herwaarden2014predicting, zhou2015chord}, and deep
feedforward networks \parencite{cherla2014multiple,
liang2015contentaware, dawson2018keyfinding, valk2018deep}.
