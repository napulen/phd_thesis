% Copyright 2022 Néstor Nápoles López

As researchers scaled the depth of \gls{cnn} models, they
noticed that it was increasingly difficult to train the
larger models effectively. One strategy that helped to
mitigate this problem was the use of \emph{residual}
connections. A residual connection consists of a connection
between a layer of the network and a deeper one,
``skipping'' some layers in between (i.e., the two layers
are not contiguous). This pattern of connections has allowed
researchers to design much deeper networks more effectively,
as the residual connections strengthen the signal of earlier
representations in deeper layers of the network. One of the
most well-known models of this kind is perhaps the
\emph{ResNet} by \textcite{he2016deep}.
