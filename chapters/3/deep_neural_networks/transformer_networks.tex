% Copyright 2021 Néstor Nápoles López

% This is \refsubsec{transformernetworks}, which introduces
% the transformer networks.

The Transformer network is a sequential model introduced by
\textcite{vaswani2017attention}. The Transformer differs
from other sequential architectures (e.g., RNN) because it
``relies entirely on an attention mechanism to draw global
dependencies between input and output''. The Transformer architecture became a fundamental methodology in natural language processing after it reached state-of-the-art performance in comparison to \glspl{rnn}. Nowadays, this architecture has been widely adopted for different problems, including Roman numeral analysis.
