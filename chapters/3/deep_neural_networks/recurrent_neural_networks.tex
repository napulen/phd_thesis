% Copyright 2021 Néstor Nápoles López

% This is \refsubsec{recurrentneuralnetworks}, which
% introduces the recurrent neural networks.

\glspl{rnn} are a type of neural networks designed to deal
with sequential data. Unlike most other neural network
architectures, \glspl{rnn} do not assume that inputs are
independent from each other. Instead, they update their
parameters considering not only the current input to the
network but also the previous inputs processed by the
network. Inputs are thus arranged as \emph{sequences} of
inputs.

\glspl{rnn} were introduced after the backpropagation
algorithm \parencite{rumelhart1988learning} was extended
into the \gls{bptt} algorithm, around 1988
\parencite{werbos1988generalization,
werbos1990backpropagation}. Nevertheless, the difficulty of
training such \gls{rnn} architectures made them unfeasible
in practical applications before the invention of the
\gls{lstm} architecture \parencite{hochreiter1997long}. They
were popularized during the \emph{deep learning} wave of
research and, since then, applied to many tasks involving
sequential data.

Throughout the years, different strategies have been
proposed to design \glspl{rnn}, for example, connecting the
output of one time step into the next time step (Jordan
\gls{rnn}), connecting the hidden state of one time step to
the next (Elman \gls{rnn}), and training the network in both
directions \parencite{schuster1997bidirectional}. It is
common to see several of these techniques combined in a
single architecture. For example, the \gls{cblstm} by
\textcite{vogl2017drum}.

In \gls{mir}, \glspl{rnn} and hybrid \glspl{crnn} have been
applied in numerous tasks. For example, onset detection
\parencite{eyben2010universal}, chord recognition
\parencite{boulangerlewandowski2013audio, sigtia2016endend,
sears2018evaluating}, voice separation
\parencite{huang2014singingvoice}, music transcription
\parencite{sigtia2014rnnbased}, tempo estimation
\parencite{bock2015accurate}, beat and downbeat tracking
\parencite{bock2016joint, krebs2016downbeat}, music
generation \parencite{liu2016predicting, liang2017automatic,
lim2017chord}, music transcription
\parencite{rigaud2016singing, sigtia2016endend,
southall2016automatic, vogl2016recurrent,
southall2017automatic, vogl2017drum, basaran2018main},
\gls{omr} \parencite{calvozaragoza2017onestep,
wel2017optical, calvozaragoza2018cameraprimus}, sequence
modelling \parencite{ycart2017study}, mood detection
\parencite{delbouys2018music}, and instrument recognition
\parencite{gururani2018instrument}.



% Recurrent Neural Networks (\glspl{rnn}) are a special type
% of ANNs where the inputs are not considered to be
% independent but part of a sequence. This distinction
% allows the network to model time-varying processes and
% tasks that require sequence-to-sequence models (e.g.,
% language, music, and weather). An additional benefit of
% \gls{rnn} archictures is that, unlike other types of deep
% learning networks such as Convolutional Neural Networks
% (CNNs), \glspl{rnn} can process inputs of arbitrary
% length. This has made CNNs the most robust choice for
% fixed-size grid-like data (e.g., images) and \glspl{rnn} a
% popular choice for tasks where the length of the input is
% unknown.

% Although modern \glspl{rnn} have achieved state-of-the-art
% accuracy in many tasks that involve sequential inputs,
% this was not easily achieved and required many years of
% innovations. For example, \glspl{rnn} were famously
% unstable and difficult to train.

% \guide{Training \glspl{rnn}}

% Over the years, \glspl{rnn} have gained a reputation as
% architectures that are difficult to train
% \parencite{pascanu2013difficulty}, given that many
% attempts of training such models have failed. For example,
% in 1994, experiments showed that as the span of
% dependencies that need to be captured by the \gls{rnn}
% increases, the probability of successfully training the
% network via the Stochastic Gradient Descent (SGD)
% optimizer rapidly reaches 0 for sequences of 10 or 20 time
% steps \parencite{bengio1994learning}.

% The main difficulty of training an \gls{rnn} is something
% known as the \emph{vanishing} and \emph{exploding}
% gradients. When the gradients vanish---something very
% common---the network is unable to learn long-term
% dependencies and can only model the dependencies of inputs
% that were provided a few time steps in the past (rather
% than modeling the entire sequence, which is usually the
% goal). When the gradients explode---something less common
% but more damaging---the convergence of the training is
% compromised because the large value of the gradient
% occludes the search of a local minimum by the optimizer
% algorithm.

% One of the reasons why \glspl{rnn} are more prone to
% vanishing and exploding gradients is their heavy
% reliability on \emph{parameter sharing}. Other
% architectures (e.g. CNNs) also share parameters between
% different neurons in order to reduce the number of
% parameters and, hence, the effort of training the network.
% \glspl{rnn}, however, make a heavier use of parameter
% sharing, as their input is usually a fixed-size vector
% designed to receive the time steps of the input sequence,
% one at a time. This design decision implies two things: 1)
% the parameters of the network are shared for every input
% of the sequence, 2) the parameters need to be updated for
% every time step of every input sequence.

% The second of these design decisions is what mostly
% contributes to the vanishing and exploding gradients. In
% other architectures, it is expected that the parameters
% will be updated once per training example, while in
% \glspl{rnn} the parameters are updated several times per
% training example.

% When the \gls{rnn} is updating its shared parameters, it
% is very easy that these parameters grow out of control
% (explode) or become close to zero (vanishing), similar to
% how a scalar number would explode or vanish when raised by
% a large exponent (e.g., $10^{1000}$ or $0.5^{1000}$).

% By making use of modern solutions, however, \glspl{rnn}
% have been able to successfully learn long-term
% dependencies (in the order of hundreds or thousands of
% time steps \parencite{hochreiter1997long}), making them
% practical for many tasks. There have been many proposed
% solutions \parencite{elhihi1995hierarchical,
% yildiz2012revisiting, jaeger2012long}, however, the most
% successful ones are the \emph{gated \glspl{rnn}}.
% Particularly, the Long Short-Term Memory (\gls{lstm}) and
% Gated \gls{gru} architectures.
