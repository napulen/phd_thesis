% Copyright 2021 Néstor Nápoles López

% This is \refsubsec{convolutionalneuralnetworks}, which
% introduces the convolutional neural networks.

\glspl{cnn} were introduced during the \emph{connectionist}
wave of research on neural networks, in
\textcite{lecun1989generalization, lecun1989handwritten}. An
important innovation of \glspl{cnn} is the idea of
\emph{shared parameters}. In a traditional feedforward
network, for example, a \gls{mlp}, every neuron of the
network is typically connected to another neuron of the
following layer using a unique parameter (i.e., a parameter
used exclusively for connecting these two neurons). This
gives the network more expressive power and the capability
of modelling very complex functions. However, it also
produces a combinatorial explosion of parameters as the
neural network grows in number of layers and neurons.
Training large networks with many unique parameters may be
unfeasible (or even impossible) due to the limitations in
memory and computing power of modern technology.
\glspl{cnn}, by design, reuse parameters throughout the
network. Thus, reducing the number of parameters compared to
a fully-connected, feedforward network.

Sharing parameters is not only important for reducing the
training time of the network, it is a bio-inspired design
motivated by the mechanics of the visual system. It is
customary, for example, to refer to the collection of
neurons that make use of the same parameter as the
\emph{receptive field} of the parameter, a term taken from
neurophysiology.

The shared parameters are modelled through a \emph{kernel}
vector. The kernel vector multiplies the inputs of the
neural network layer in a way that resembles the
mathematical operation of \emph{convolution}, which
motivated the use of the term \emph{Convolutional Neural
Networks}. After training, it is assumed that each of those
kernels will learn a low-level, localized, feature, which is
going to be searched across the entire input vector of the
network and propagated into deeper (higher-level) kernels of
the network.

Given the way that convolutional kernels work, \glspl{cnn}
have become the standard methodology for dealing with
fixed-length, grid-like structures (e.g., images), producing
state-of-the-art performance in many tasks. For example,
they have systematically been the state-of-the-art in the
ImageNet challenge since 2012
\parencite{krizhevsky2012imagenet}, identifying over 1,000
classes of objects in an image.

In \gls{mir}, \glspl{cnn} have been used for genre
recognition \parencite{dieleman2011audiobased}, chord
recognition \parencite{humphrey2012rethinking}, structural
analysis \parencite{ullrich2014boundary, grill2015music},
music tagging \parencite{choi2016automatic}, instrument
recognition \parencite{lostanlen2016deep}, \gls{omr}
\parencite{calvozaragoza2017endend, pacha2018optical}, beat
tracking \parencite{gkiokas2017convolutional}, source
separation \parencite{miron2017monaural}, syllable
segmentation \parencite{pons2017scoreinformed}, key
detection \parencite{korzeniowski2018genreagnostic}, and
tempo estimation \parencite{schreiber2018singlestep,
schreiber2019musical}.
