% Copyright 2021 Néstor Nápoles López

% This is \refsubsubsec{lstm}, which introduces the lstm.

The \gls{lstm} architecture was introduced by
\textcite{hochreiter1997long}. Since its introduction, it
has been increasingly used to approach multiple problems.
\gls{lstm} architectures extend the basic structure of
\glspl{rnn} and it is relatively safe to assume that any
problem that involves an \gls{rnn} can substitute the
\gls{rnn} with an \gls{lstm}, matching (or often improving)
the results obtained by the \gls{rnn}.

The main contribution of \glspl{lstm} is to provide a
solution to \emph{vanishing gradients}. Given that vanishing
gradients are associated with the loss of long-term
dependencies, mitigating this issue would enable, in theory,
the capability of the network to learn long-term
dependencies between the inputs. In order to achieve these
long-term dependencies, an \gls{lstm} substitutes the
regular \gls{rnn} unit with a more complex one. This new,
more complex unit, receives the name of an \emph{\gls{lstm}
unit}.

The main difference between a regular \gls{rnn} unit and an
\gls{lstm} unit is the addition of a cell and three gates:
the forget gate, the input gate, and the output gate. In
conjunction, the cell and the gates allow the network to
control the flow of the information, selecting what
information should the network store and what information
should it forget. The gates of the \gls{lstm} unit act as
``regulators'' and each of them is responsible of a
different part of the network. The forget gate controls what
information of previous time steps in the network should be
kept and what should be forgotten. The input gate regulates
how much of the new inputs to the network should enter the
cell. The output gate controls how much of previous time
steps in the network should be used for computing the
activation of the output of the network.

Given that \glspl{rnn} and more specifically \glspl{lstm}
are useful architectures for modelling sequence-to-sequence
processes, they can be useful for musical applications, for
example, detecting the changes of key in a symbolic music
file (modulation).

% memory cell is that the memory cell has gates that allow
% the gradient of the unit to be controlled and always norm
% 1. These gates keep the value of the memory cell under
%    control.

Further improvements on \glspl{lstm} have extended the
elements of the \gls{lstm}, for example, adding a forget
cell, which allows the memory cell to drop information that
is no longer relevant.

\guide{Clipping} One of the most successful techniques for
controlling the vanishing/exploding gradients problem.

The gradient is computed as usual, whenever that gradient
exceeds a threshold value, it is clipped.

There is a mathematical proof of why this works, developed
by Tenescu.

\guide{Modern \glspl{rnn}} Together with the improvements on
the vanishing/exploding gradients problem, other researchers
have also contributed with other ideas.

For example, during the same year that the \gls{lstm} was
proposed, another paper proposed a bidirectional \gls{rnn}
that could be used in offline systems (as it requires
knowledge of the future, it can’t be used in real-time and
other online applications).

These advances in different fronts have made not only
addressed problems related with training but also improved
the capabilities and performance of \glspl{rnn}, many of
them are not mutually exclusive with each other, so they can
be combined.

Therefore, it is common to see now architectures that
combine these approaches, for example, \gls{blstm} networks.

These type of models have been successfully trained and
provided state-of-the-art in multiple tasks.
