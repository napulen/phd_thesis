% Copyright 2021 Néstor Nápoles López

% This is \refsubsubsec{lstm}, which introduces the lstm.

The Long Short-Term Memory (LSTM) architecture was
introduced in 1997 by Hochreiter and Schmidhuber
\parencite{hochreiter1997long}. Since its introduction, the
LSTM has become an increasingly popular architecture, which
has been used in many tasks and problems. LSTM architectures
extend the basic structure of RNNs and it is relatively safe
to assume that any problem that involves an RNN can
substitute the RNN with an LSTM, matching or even improving
the results.

The main contribution of LSTMs is to try to provide a
solution to the issue of vanishing gradients. Given that
vanishing gradients are associated with the loss of
long-term dependencies, mitigating this issue would enable,
in theory, the capability of the network to learn long-term
dependencies between the inputs. In order to achieve these
long-term dependencies, an LSTM substitutes the regular RNN
unit with a more complex one. This new, more complex unit,
receives the name of an \emph{LSTM unit}.

The main difference between a regular RNN unit and an LSTM
unit is the addition of a cell and three gates: the forget
gate, the input gate, and the output gate. In conjunction,
the cell and the gates allow the network to control the flow
of the information, selecting what information should the
network store and what information should it forget. The
gates of the LSTM unit act as ``regulators'' and each of
them is responsible of a different part of the network. The
forget gate controls what information of previous time steps
in the network should be kept and what should be forgotten.
The input gate regulates how much of the new inputs to the
network should enter the cell. The output gate controls how
much of previous time steps in the network should be used
for computing the activation of the output of the network.

Given that RNNs and more specifically LSTMs are useful
architectures for modelling sequence-to-sequence processes,
they can be useful for musical applications, for example,
detecting the changes of key in a symbolic music file
(modulation).

% memory cell is that the memory cell has gates that allow
% the gradient of the unit to be controlled and always norm
% 1. These gates keep the value of the memory cell under
%    control.

Further improvements on LSTMs have extended the elements
of the LSTM, for example, adding a forget cell, which
allows the memory cell to drop information that is no
longer relevant.

\guide{Clipping} One of the most successful techniques for
controlling the vanishing/exploding gradients problem.

The gradient is computed as usual, whenever that gradient
exceeds a threshold value, it is clipped.

There is a mathematical proof of why this works, developed
by Tenescu.

\guide{Modern RNNs} Together with the improvements on the
vanishing/exploding gradients problem, other researchers
have also contributed with other ideas.

For example, during the same year that the LSTM was
proposed, another paper proposed a bidirectional RNN that
could be used in offline systems (as it requires knowledge
of the future, it can’t be used in real-time and other
online applications).

These advances in different fronts have made not only
addressed problems related with training but also improved
the capabilities and performance of RNNs, many of them are
not mutually exclusive with each other, so they can be
combined.

Therefore, it is common to see now architectures that
combine these approaches, for example, BLSTM
(Bidirectional Long Short-Term Memory) networks.

These type of models have been successfully trained and
provided state-of-the-art in multiple tasks.
