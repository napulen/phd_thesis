% Copyright 2021 Néstor Nápoles López

% This is \refsubsubsec{lstm}, which introduces the lstm.

The \gls{lstm} architecture was introduced by
\textcite{hochreiter1997long}. Since its introduction, it
has been increasingly used to approach multiple problems.
\gls{lstm} architectures extend the basic structure of
\glspl{rnn} and it is relatively safe to assume that any
problem that involves an \gls{rnn} can substitute the
\gls{rnn} with an \gls{lstm}, matching (or often improving)
the results obtained by the \gls{rnn}.

The main contribution of \glspl{lstm} is to provide a
solution to \emph{vanishing gradients}. Given that vanishing
gradients are associated with the loss of long-term
dependencies, mitigating this issue would enable, in theory,
the capability of the network to learn long-term
dependencies between the inputs. In order to achieve these
long-term dependencies, an \gls{lstm} substitutes the
regular \gls{rnn} unit with a more complex one. This new,
more complex unit, receives the name of an \emph{LSTM unit}.

The main difference between an \gls{rnn} and \gls{lstm}
units is that the \gls{lstm} adds a cell and three gates:
the ``forget'' gate, the ``input'' gate, and the ``output''
gate. In conjunction, the cell and gates allow the network
to control the flow of the information, selecting what
information should the network store and what information
should it forget. The gates of the \gls{lstm} unit act as
``regulators'' and each of them is responsible of a
different part of the network. The forget gate controls what
information of previous time steps in the network should be
kept and what should be forgotten. The input gate regulates
how much of the new inputs to the network should enter the
cell. The output gate controls how much of previous time
steps in the network should be used for computing the
activation of the output of the network.

Beside the \glspl{lstm}, further improvements have also been
proposed to \glspl{rnn}. For example, during the same year
that the \gls{lstm} was proposed, another paper proposed a
bidirectional \gls{rnn} that could be used in offline
systems (as it requires knowledge of the future, it can’t be
used in \emph{realtime} applications). These advances have
improved the capabilities of \glspl{rnn}, as they are often
not mutually exclusive and can be used in combination.
Therefore, it is common to see nowadays architectures that
combine these approaches, for example, \gls{blstm} networks.
These type of models have been successful and provided
state-of-the-art in multiple tasks. Given that \glspl{rnn}
and more specifically \glspl{lstm} are useful architectures
for modelling sequence-to-sequence processes, they can be
useful for musical applications.
