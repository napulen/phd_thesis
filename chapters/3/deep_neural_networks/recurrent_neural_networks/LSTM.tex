% Copyright 2021 Néstor Nápoles López

% This is \refsubsubsec{lstm}, which introduces the lstm.

The \gls{lstm} architecture was introduced by
\textcite{hochreiter1997long}. Since its introduction, it
has been increasingly used to approach multiple problems.
\gls{lstm} architectures extend the basic structure of
\glspl{rnn} and it is relatively safe to assume that any
problem that involves an \gls{rnn} can substitute the
\gls{rnn} with an \gls{lstm}, matching (or often improving)
the results obtained by the \gls{rnn}.

The main contribution of \glspl{lstm} is to provide a
solution to \emph{vanishing gradients}.\footnote{Vanishing
and exploding gradients are undesirable effects during the
training of a neural network. They consist of either
negligible (vanishing) or very large (exploding) updates to
the trainable parameters. In the first instance, it leads to
no learning, in the second instance, it leads to numerical
instability. This effect was prominent particularly among
Recurrent Neural Networks, and it made them difficult to
train. See \textcite{bengio1994learning} for further
information on this topic.} Given that vanishing gradients
are associated with the loss of long-term dependencies,
mitigating this issue would enable, in theory, the
capability of the network to learn long-term dependencies
between the inputs. In order to achieve these long-term
dependencies, an \gls{lstm} substitutes the regular
\gls{rnn} unit with a more complex one. This new, more
complex unit, receives the name of an \emph{LSTM unit}.

The main difference between an \gls{rnn} and \gls{lstm}
units is that the \gls{lstm} adds a cell and three gates:
the ``forget'' gate, the ``input'' gate, and the ``output''
gate. In conjunction, the cell and gates allow the network
to control the flow of the information, selecting what
information should the network store and what information
should it forget. The gates of the \gls{lstm} unit act as
``regulators'' and each of them is responsible of a
different part of the network. The forget gate controls what
information of previous time steps in the network should be
kept and what should be forgotten. The input gate regulates
how much of the new inputs to the network should enter the
cell. The output gate controls how much of previous time
steps in the network should be used for computing the
activation of the output of the network.
