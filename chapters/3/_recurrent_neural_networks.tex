% Taken verbatim from comps Q9

% Certain phenomena (music, for example) have a strong
% dependence on time.

% Explaining the meaning of a given input (for example, a
% note) does not only depend on the input, but on the time
% where that input occurs.

% In the research related with Artificial Neural Networks,
% researchers have tried to investigate these type of
% phenomena using Recurrent Neural Networks (RNN).

% An RNN is trained with sequences of inputs, rather than
% just inputs.

% They are useful for many different problems.

Recurrent Neural Networks (RNNs) are a special type of ANNs
(see Question 2 for more information on ANNs) where the
inputs are not considered to be independent but part of a
sequence. This distinction allows the network to model
time-varying processes and tasks that require
sequence-to-sequence models (e.g., language, music, and
weather). An additional benefit of RNN archictures is that,
unlike other types of deep learning networks such as
Convolutional Neural Networks (CNNs), RNNs can process
inputs of arbitrary length. This has made CNNs the most
robust choice for fixed-size grid-like data (e.g., images)
and RNNs a popular choice for tasks where the length of the
input is unknown.

Although modern RNNs have achieved state-of-the-art accuracy
in many tasks that involve sequential inputs, this was not
easily achieved and required many years of innovations. The
most important aspect in which RNNs have seemed to take more
time to progress in comparison to other deep learning
techniques, is the successful training of the networks,
which seemed to be very unstable and remains one of the
biggest challenges of research in RNNs.

\guide{Training RNNs}

Over the years, RNNs have gained a reputation as
architectures that are very difficult to train
\parencite{pascanu2013difficulty}, given that many attempts
of training such models have failed. For example, in 1994,
experiments showed that as the span of dependencies that
need to be captured by the RNN increases, the probability of
successfully training the network via the Stochastic
Gradient Descent (SGD) optimizer rapidly reaches 0 for
sequences of 10 or 20 time steps
\parencite{bengio1994learning}.

The main difficulty of training an RNN is something known as
the \emph{vanishing} and \emph{exploding} gradients. When
the gradients vanish---something very common---the network
is unable to learn long-term dependencies and can only model
the dependencies of inputs that were provided a few time
steps in the past (rather than modelling the entire
sequence, which is usually the goal). When the gradients
explode---something less common but more damaging---the
convergence of the training is compromised because the large
value of the gradient occludes the search of a local minimum
by the optimizer algorithm.

One of the reasons why RNNs are more prone to vanishing and
exploding gradients is their heavy reliability on
\emph{parameter sharing}. Other architectures (e.g. CNNs)
also share parameters between different neurons in order to
reduce the number of parameters and, hence, the effort of
training the network. RNNs, however, make a heavier use of
parameter sharing, as their input is usually a fixed-size
vector designed to receive the time steps of the input
sequence, one at a time. This design decision implies two
things: 1) the parameters of the network are shared for
every input of the sequence, 2) the parameters need to be
updated for every time step of every input sequence.

The second of these design decisions is what mostly
contributes to the vanishing and exploding gradients. In
other architectures, it is expected that the parameters will
be updated once per training example, while in RNNs the
parameters are updated several times per training example.

When the RNN is updating its shared parameters, it is very
easy that these parameters grow out of control (explode) or
drop to zero really fast (vanishing), similar to how a
scalar number would explode or vanish if raised by a very
large exponent (e.g., $0.5^{1000}$ or $10^{1000}$).

By making use of modern solutions, however, RNNs have been
able to successfully learn long-term dependencies (in the
order of hundreds or thousands of time steps
\parencite{hochreiter1997long}), making them practical for
many tasks. There have been many proposed solutions
\parencite{elhihi1995hierarchical, yildiz2012revisiting,
jaeger2012long}, however, the most successful ones are the
\emph{gated RNNs}. Particularly, a type of gated RNN known
as the Long Short-Term Memory (LSTM) architecture.

\guide{Long Short-Term Memory (LSTM)}

The Long Short-Term Memory architecture was introduced in
1997 by Hochreiter and Schmidhuber
\parencite{hochreiter1997long}. Since its introduction, the
LSTM has become an increasingly popular architecture, which
has been used in many tasks and problems. LSTM architectures
extend the basic structure of RNNs and it is relatively safe
to assume that any problem that involves an RNN can
substitute the RNN with an LSTM, matching or even improving
the results.

The main contribution of LSTMs is to try to provide a
solution to the issue of vanishing gradients. Given that
vanishing gradients are associated with the loss of
long-term dependencies, mitigating this issue would enable,
in theory, the capability of the network to learn long-term
dependencies between the inputs. In order to achieve these
long-term dependencies, an LSTM substitutes the regular RNN
unit with a more complex one. This new, more complex unit,
receives the name of an \emph{LSTM unit}.

The main difference between a regular RNN unit and an LSTM
unit is the addition of a cell and three gates: the forget
gate, the input gate, and the output gate. In conjunction,
the cell and the gates allow the network to control the flow
of the information, selecting what information should the
network store and what information should it forget. The
gates of the LSTM unit act as ``regulators'' and each of
them is responsible of a different part of the network. The
forget gate controls what information of previous time steps
in the network should be kept and what should be forgotten.
The input gate regulates how much of the new inputs to the
network should enter the cell. The output gate controls how
much of previous time steps in the network should be used
for computing the activation of the output of the network.

Given that RNNs and more specifically LSTMs are useful
architectures for modelling sequence-to-sequence processes,
they can be useful for musical applications, for example,
detecting the changes of key in a symbolic music file
(modulation).

% memory cell is that the memory cell has gates that allow
% the gradient of the unit to be controlled and always norm
% 1. These gates keep the value of the memory cell under
%    control.

% Further improvements on LSTMs have extended the elements
% of the LSTM, for example, adding a forget cell, which
% allows the memory cell to drop information that is no
% longer relevant.

% \guide{Clipping} One of the most successful techniques for
% controlling the vanishing/exploding gradients problem.

% The gradient is computed as usual, whenever that gradient
% exceeds a threshold value, it is clipped.

% There is a mathematical proof of why this works, developed
% by Tenescu.

% \guide{Modern RNNs} Together with the improvements on the
% vanishing/exploding gradients problem, other researchers
% have also contributed with other ideas.

% For example, during the same year that the LSTM was
% proposed, another paper proposed a bidirectional RNN that
% could be used in offline systems (as it requires knowledge
% of the future, it canâ€™t be used in real-time and other
% online applications).

% These advances in different fronts have made not only
% addressed problems related with training but also improved
% the capabilities and performance of RNNs, many of them are
% not mutually exclusive with each other, so they can be
% combined.

% Therefore, it is common to see now architectures that
% combine these approaches, for example, BLSTM
% (Bidirectional Long Short-Term Memory) networks.

% These type of models have been successfully trained and
% provided state-of-the-art in multiple tasks.


\guide{Designing an LSTM for detecting modulation}

In order to design a system for the detection of
``modulations'', the first step is to define the
expectations of such a system in terms of its inputs and
outputs. Particularly, given that the term ``modulation'' is
very difficult to define, I will consider referring to it as
a \emph{local key} finder instead (see Question
\ref{chap:chap6} for a further discussion on modulations and
local keys).

A local key-finding system produces an output at every time
step. The output consists of one of the 24 major and minor
keys. That is, the local key for any given time step. The
time step units consist of onset events in the symbolic
music input (i.e., attacks). The inputs of the system could
be the range of valid MIDI note numbers (0-127) in a one-hot
encoding scheme, yielding a fixed input vector of dimension
128 for every time step. Nevertheless, given that some
local-key-finding models have been successfully implemented
without octave information
\parencite{napoleslopez2019keyfinding}, the input vector can
be substituted by a 12-dimensional pitch-class vector in a
one-hot encoding scheme. This reduces the number of
parameters by an order of magnitude and, therefore, is
expected to reduce the computational cost of training the
model.

The system consists of a single hidden layer with a
recurrent LSTM unit. The gates of the LSTM unit use a
sigmoid function as their non-linear activation function and
the input to the network uses a \emph{Rectified linear unit}
(ReLU) as its activation function.

As a proof of concept and given the scarce data available
for training the network, an existing probabilistic model
that requires no training
\parencite{napoleslopez2019keyfinding} is used for
generating local key annotations in a corpus of 892 MIDI
files of classical music. The success criteria of this LSTM
system is, therefore, to provide a similar local-key
segmentation on unseen MIDI examples as the segmentation
provided by the baseline, probabilistic model.
