
@phdthesis{gomez_tonal_2006,
	type = {{PhD} {Thesis}},
	title = {Tonal {Description} of {Music} {Audio} {Signals}},
	school = {Universitat Pompeu Fabra},
	author = {Gómez, Emilia},
	year = {2006},
	keywords = {Audio Chord Estimation, Audio Feature Extractor, Q1, Tonality, High priority},
	file = {Gómez - 2006 - Tonal description of music audio signals.pdf:C\:\\Users\\nesto\\Zotero\\storage\\T7CQRJZT\\Gómez - 2006 - Tonal description of music audio signals.pdf:application/pdf;Snapshot:C\:\\Users\\nesto\\Zotero\\storage\\6ETU9XKN\\472.html:text/html}
}

@inproceedings{eyben_opensmile:_2010,
	address = {New York, NY, USA},
	series = {{MM} '10},
	title = {Opensmile: {The} {Munich} {Versatile} and {Fast} {Open}-source {Audio} {Feature} {Extractor}},
	isbn = {978-1-60558-933-6},
	shorttitle = {Opensmile},
	url = {http://doi.acm.org/10.1145/1873951.1874246},
	doi = {10.1145/1873951.1874246},
	abstract = {We introduce the openSMILE feature extraction toolkit, which unites feature extraction algorithms from the speech processing and the Music Information Retrieval communities. Audio low-level descriptors such as CHROMA and CENS features, loudness, Mel-frequency cepstral coefficients, perceptual linear predictive cepstral coefficients, linear predictive coefficients, line spectral frequencies, fundamental frequency, and formant frequencies are supported. Delta regression and various statistical functionals can be applied to the low-level descriptors. openSMILE is implemented in C++ with no third-party dependencies for the core functionality. It is fast, runs on Unix and Windows platforms, and has a modular, component based architecture which makes extensions via plug-ins easy. It supports on-line incremental processing for all implemented features as well as off-line and batch processing. Numeric compatibility with future versions is ensured by means of unit tests. openSMILE can be downloaded from http://opensmile.sourceforge.net/.},
	urldate = {2019-09-16},
	booktitle = {Proceedings of the 18th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Eyben, Florian and Wöllmer, Martin and Schuller, Björn},
	year = {2010},
	note = {event-place: Firenze, Italy},
	keywords = {Audio Feature Extractor, Q1},
	pages = {1459--1462},
	file = {ACM Full Text PDF:C\:\\Users\\nesto\\Zotero\\storage\\75PAMXS5\\Eyben et al. - 2010 - Opensmile The Munich Versatile and Fast Open-sour.pdf:application/pdf}
}

@inproceedings{lartillot_matlab_2007,
	address = {Bordeaux, France},
	title = {A {Matlab} {Toolbox} for {Musical} {Feature} {Extraction} {From} {Audio}},
	booktitle = {International {Conference} on {Digital} {Audio} {Effects}},
	author = {Lartillot, Olivier and Toiviainen, Petri},
	year = {2007},
	keywords = {Audio Feature Extractor, Q1},
	pages = {237--244},
	file = {Full Text:C\:\\Users\\nesto\\Zotero\\storage\\J329HQUT\\Lartillot and Toiviainen - 2007 - A Matlab toolbox for musical feature extraction fr.pdf:application/pdf}
}

@inproceedings{moffat_evaluation_2015,
	address = {Trondheim, Norway},
	title = {An {Evaluation} of {Audio} {Feature} {Extraction} {Toolboxes}},
	booktitle = {Proceedings of the 18th {International} {Conference} on {Digital} {Audio} {Effects}},
	author = {Moffat, David and Ronan, David and Reiss, Joshua D.},
	year = {2015},
	keywords = {Audio Feature Extractor, Q1, High priority, Read},
	annote = {Summary
10 audio feature extraction toolboxes evaluated according to the Cranfield Model for evaluation of information retrieval systems. The toolboxes were selected according to popularity, programming environment range, and having recent updates.
The Cranfield model evaluates 6 aspects:

Coverage
Time Lag
Effort
Presentation
Precision
Recall

(From all of these evaluations, I really only care about "Coverage" for Key Estimation models)
The first 4 are evaluated in the paper, precision and recall are not discussed. However, they are discussed in "the scientific evaluation of music information retrieval systems: foundations and future".
The accuracy of audio features is out of the scope of the paper but it is described in "mir\_eval: A transparent implementation of common MIR metrics".
 
Low-level features are computed directly on the audio data, hold little perceptual relevance. High-level features hold a greater semantic meaning. Key is considered a high-level feature.
MPEG-7 standard defines 17 low-level descriptors (LLD).
The CUIDADO project defines 54 audio features.
A comprehensive classification of audio features is described in "A large set of audio features for sound description" and "Features for content-based audio retrieval".
Essentia provides the largest range of features and 100\% coverage of the MPEG-7 audio descriptors.
Pre-processing of audio allows the user to correct and compensate for low-quality or degraded audio.
 
Final Summary:
From this paper I obtained

2 references to taxonomies of audio features
Characteristics of low-level and high-level features
Categorization of key as a high-level feature
2 references regarding evaluation of MIR systems
2 references regarding ablation tests/frameworks for audio features

 
 },
	file = {Full Text:C\:\\Users\\nesto\\Zotero\\storage\\PGWEUPZN\\Moffat et al. - 2015 - An evaluation of audio feature extraction toolboxe.pdf:application/pdf}
}

@inproceedings{bogdanov_essentia:_2013,
	address = {Curitiba, Brazil},
	title = {Essentia: {An} audio analysis library for music information retrieval},
	shorttitle = {Essentia},
	booktitle = {Proceedings of the 14th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	publisher = {International Society for Music Information Retrieval (ISMIR)},
	author = {Bogdanov, Dmitry and Wack, Nicolas and Gómez Gutiérrez, Emilia and Gulati, Sankalp and Boyer, Herrera and Mayor, Oscar and Roma Trepat, Gerard and Salamon, Justin and Zapata González, José Ricardo and Serra, Xavier},
	year = {2013},
	keywords = {Audio Feature Extractor, Q1},
	pages = {493--8},
	file = {Full Text:C\:\\Users\\nesto\\Zotero\\storage\\K8RTBQWY\\Bogdanov et al. - 2013 - Essentia An audio analysis library for music info.pdf:application/pdf;Snapshot:C\:\\Users\\nesto\\Zotero\\storage\\YT6JHS6B\\32252.html:text/html}
}

@inproceedings{tzanetakis_pitch_2002,
	title = {Pitch {Histograms} in {Audio} and {Symbolic} {Music} {Information} {Retrieval}},
	doi = {10.1076/jnmr.32.2.143.16743},
	abstract = {In order to represent musical content, pitch and timing information is utilized in the majority of existing work in Symbolic Music Information Retrieval (MIR). Symbolic representations such as MIDI allow the easy calculation of such information and its manipulation. In contrast, most of the existing work in Audio MIR uses timbral and beat information, which can be calculated using automatic computer audition techniques. In this paper, Pitch Histograms are defined and proposed as a way to represent the pitch content of music signals both in symbolic and audio form. This representation is evaluated in the context of automatic musical genre classification. A multiple-pitch detection algorithm for polyphonic signals is used to calculate Pitch Histograms for audio signals. In order to evaluate the extent and significance of errors resulting from the automatic multiple-pitch detection, automatic musical genre classification results from symbolic and audio data are compared. The comparison indicates that Pitch H...},
	booktitle = {Proceedings of the 3rd {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Tzanetakis, George and Ermolinskiy, Andrey and Cook, Perry R.},
	year = {2002},
	keywords = {Q1, Symbolic and Audio}
}

@article{velarde_convolution-based_2018,
	title = {Convolution-{Based} {Classification} of {Audio} and {Symbolic} {Representations} of {Music}},
	volume = {47},
	issn = {0929-8215},
	url = {https://doi.org/10.1080/09298215.2018.1458885},
	doi = {10.1080/09298215.2018.1458885},
	abstract = {We present a novel convolution-based method for classification of audio and symbolic representations of music, which we apply to classification of music by style. Pieces of music are first sampled to pitch–time representations (spectrograms or piano-rolls) and then convolved with a Gaussian filter, before being classified by a support vector machine or by k-nearest neighbours in an ensemble of classifiers. On the well-studied task of discriminating between string quartet movements by Haydn and Mozart, we obtain accuracies that equal the state of the art on two data-sets. However, in multi-class composer identification, methods specialised for classifying symbolic representations of music are more effective. We also performed experiments on symbolic representations, synthetic audio and two different recordings of The Well-Tempered Clavier by J. S. Bach to study the method’s capacity to distinguish preludes from fugues. Our experimental results show that our approach performs similarly on symbolic representations, synthetic audio and audio recordings, setting our method apart from most previous studies that have been designed for use with either audio or symbolic data, but not both.},
	number = {3},
	urldate = {2019-09-16},
	journal = {Journal of New Music Research},
	author = {Velarde, Gissel and Chacón, Carlos Cancino and Meredith, David and Weyde, Tillman and Grachten, Maarten},
	month = may,
	year = {2018},
	keywords = {Q1, Symbolic and Audio},
	pages = {191--205},
	file = {Full Text PDF:C\:\\Users\\nesto\\Zotero\\storage\\XBWM52F2\\Velarde et al. - 2018 - Convolution-based classification of audio and symb.pdf:application/pdf;Snapshot:C\:\\Users\\nesto\\Zotero\\storage\\BPPJ8F47\\09298215.2018.html:text/html}
}

@inproceedings{lidy_improving_2007,
	title = {Improving {Genre} {Classification} by {Combination} of {Audio} and {Symbolic} {Descriptors} {Using} a {Transcription} {System}},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Music} {Information} {Retrieval}},
	author = {Lidy, Thomas and Rauber, Andreas and Pertusa, Antonio and Quereda, José Manuel Inesta},
	year = {2007},
	keywords = {Q1, Symbolic and Audio},
	pages = {61--66},
	file = {Full Text:C\:\\Users\\nesto\\Zotero\\storage\\QCHIWM9Z\\Lidy et al. - 2007 - Improving Genre Classification by Combination of A.pdf:application/pdf}
}

@article{toiviainen_visualization_2007,
	title = {Visualization of {Tonal} {Content} in the {Symbolic} and {Audio} {Domains}},
	volume = {15},
	journal = {Computing in Musicology},
	author = {Toiviainen, Petri},
	year = {2007},
	keywords = {Q1, Symbolic and Audio, High priority},
	pages = {187--199},
	file = {Full Text:C\:\\Users\\nesto\\Zotero\\storage\\RQVDUUJF\\Toiviainen - 2007 - Visualization of Tonal Content in the Symbolic and.pdf:application/pdf}
}

@book{hewlett_tonal_2007,
	title = {Tonal {Theory} for the {Digital} {Age}},
	isbn = {978-0-936943-17-6},
	language = {en},
	publisher = {Center for Computer Assisted Research in the Humanities},
	author = {Hewlett, Walter B. and Selfridge-Field, Eleanor and Correia, Edmund},
	year = {2007},
	note = {Google-Books-ID: cCX0AAAAMAAJ},
	keywords = {Q1, Tonality, Audio and Symbolic}
}

@inproceedings{mckay_combining_2008,
	address = {Philadelphia, USA},
	title = {Combining {Features} {Extracted} from {Audio}, {Symbolic} and {Cultural} {Sources}.},
	abstract = {This paper experimentally investigates the classification utility of combining features extracted from separate au- dio, symbolic and cultural sources of musical information. This was done via a series of genre classification experi- ments performed using all seven possible combinations and subsets of the three corresponding types of features. These experiments were performed using jMIR, a soft- ware suite designed for use both as a toolset for perform- ing MIR research and as a platform for developing and sharing new algorithms. The experimental results indicate that combining fea- ture types can indeed substantively improve classification accuracy. Accuracies of 96.8\% and 78.8\% were attained respectively on 5 and 10-class genre taxonomies when all three feature types were combined, compared to average respective accuracies of 85.5\% and 65.1\% when features extracted from only one of the three sources of data were used. It was also found that combining feature types de- creased the seriousness of those misclassifications that were made, on average, particularly when cultural features were included.},
	booktitle = {Proceedings of the  9th {International} {Conference} on {Music} {Information} {Retrieval}},
	author = {McKay, Cory and Fujinaga, Ichiro},
	month = jan,
	year = {2008},
	keywords = {Q1, Symbolic and Audio, High priority},
	pages = {597--602},
	file = {Full Text PDF:C\:\\Users\\nesto\\Zotero\\storage\\YKBAD2QM\\McKay and Fujinaga - 2008 - Combining Features Extracted from Audio, Symbolic .pdf:application/pdf}
}

@article{raphael_aligning_2006,
	title = {Aligning music audio with symbolic scores using a hybrid graphical model},
	volume = {65},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-006-8415-3},
	doi = {10.1007/s10994-006-8415-3},
	abstract = {We present a new method for establishing an alignment between a polyphonic musical score and a corresponding sampled audio performance. The method uses a graphical model containing both latent discrete variables, corresponding to score position, as well as a latent continuous tempo process. We use a simple data model based only on the pitch content of the audio signal. The data interpretation is defined to be the most likely configuration of the hidden variables, given the data, and we develop computational methodology to identify or approximate this configuration using a variant of dynamic programming involving parametrically represented continuous variables. Experiments are presented on a 55-minute hand-marked orchestral test set.},
	language = {en},
	number = {2},
	urldate = {2019-09-16},
	journal = {Machine Learning},
	author = {Raphael, Christopher},
	month = dec,
	year = {2006},
	keywords = {Q1, Audio and Symbolic},
	pages = {389--409},
	file = {Springer Full Text PDF:C\:\\Users\\nesto\\Zotero\\storage\\IRNWF9GN\\Raphael - 2006 - Aligning music audio with symbolic scores using a .pdf:application/pdf}
}

@inproceedings{wang_music_2015,
	address = {Málaga, Spain},
	title = {Music {Pattern} {Discovery} with {Variable} {Markov} {Oracle}: {A} {Unified} {Approach} to {Symbolic} and {Audio} {Representations}},
	shorttitle = {Music {Pattern} {Discovery} with {Variable} {Markov} {Oracle}},
	abstract = {This paper presents a framework for automatically discovering patterns in a polyphonic music piece. The proposed framework is capable of handling both symbolic and audio representations. Chroma features are post-processed with heuristics stemming from musical knowledge and fed into the pattern discovery framework. The pattern-finding algorithm is based on Variable Markov Oracle. The Variable Markov Oracle data structure is capable of locating repeated suffixes within a time series, thus making it an appropriate tool for the pattern discovery task. Evaluation of the proposed framework is performed on the JKU Patterns Development Dataset with state of the art performance.},
	booktitle = {Proceedings of the 16th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Wang, Cheng-i and Hsu, Jennifer and Dubnov, Shlomo},
	year = {2015},
	keywords = {Q1, Audio and Symbolic},
	file = {Full Text PDF:C\:\\Users\\nesto\\Zotero\\storage\\L4FEFT42\\Wang et al. - 2015 - Music Pattern Discovery with Variable Markov Oracl.pdf:application/pdf}
}

@inproceedings{collins_bridging_2014,
	title = {Bridging the {Audio}-{Symbolic} {Gap}: {The} {Discovery} of {Repeated} {Note} {Content} {Directly} from {Polyphonic} {Music} {Audio}},
	shorttitle = {Bridging the {Audio}-{Symbolic} {Gap}},
	url = {http://www.aes.org/e-lib/browse.cfm?elib=17096},
	abstract = {Algorithms for the discovery of musical repetition have been developed in audio and symbolic domains more or less independently for over a decade. In this paper we combine algorithms for multiple F0 estimation, beat tracking, quantisation, and pattern discovery, so that for the first time, the note content of motifs, themes, and repeated sections can be discovered directly from polyphonic music audio. Testing on deadpan and expressive piano renditions of pieces, we compared pattern discovery...},
	language = {English},
	urldate = {2019-09-16},
	booktitle = {Audio {Engineering} {Society} {Conference}: 53rd {International} {Conference}: {Semantic} {Audio}},
	publisher = {Audio Engineering Society},
	author = {Collins, Tom and Böck, Sebastian and Krebs, Florian and Widmer, Gerhard},
	month = jan,
	year = {2014},
	keywords = {Q1, Audio and Symbolic},
	file = {Collins et al. - 2014 - Bridging the Audio-Symbolic Gap The Discovery of .pdf:C\:\\Users\\nesto\\Zotero\\storage\\26BBGS9J\\Collins et al. - 2014 - Bridging the Audio-Symbolic Gap The Discovery of .pdf:application/pdf;Snapshot:C\:\\Users\\nesto\\Zotero\\storage\\QL8UXCYY\\browse.html:text/html}
}

@techreport{peeters_large_2004,
	title = {A {Large} {Set} of {Audio} {Features} for {Sound} {Description} ({Similarity} and {Classification}) in the {CUIDADO} {Project}},
	author = {Peeters, Geoffroy},
	year = {2004},
	keywords = {Audio Feature Extractor, High priority, Q1, Read},
	pages = {1--25},
	annote = {Summary
 
(poorly written, difficult to follow due to grammar and spelling mistakes)
The list of 54 low-level features},
	file = {Peeters_2003_cuidadoaudiofeatures.pdf:C\:\\Users\\nesto\\Zotero\\storage\\NDP4AX7T\\Peeters_2003_cuidadoaudiofeatures.pdf:application/pdf}
}

@inproceedings{matthias_mauch_audio_2013,
	address = {Curitiba, Brazil},
	title = {The {Audio} {Degradation} {Toolbox} and {Its} {Application} to {Robustness} {Evaluation}.},
	url = {https://zenodo.org/record/1415862#.XX_3tZNKjmE},
	urldate = {2019-09-16},
	booktitle = {Proceedings of the 14th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Matthias Mauch and Sebastian Ewert},
	month = nov,
	year = {2013},
	keywords = {Q1, High priority, Audio Degradation},
	pages = {83--88},
	annote = {Summary
 
Audio degrades for various reasons. For example, low-quality microphones, noisy recordings, mp3 compression, dynamic compression, and vinyl decay.
 
This paper presents 14 degradation tools available as MATLAB scripts.
 
The degradation tools presented can be "chained" to create more complex, "real-world" degradations.
 
4 music informatics tasks are presented, showing that performance strongly depends on the combination of degradations applied.
 
Specific degradations could reduce or reverse the performance difference between two competing methods.
 
Code, sounds, impulse responses and definitions are available for download.
 
 },
	file = {Zenodo Full Text PDF:C\:\\Users\\nesto\\Zotero\\storage\\WK4GPFB4\\Matthias Mauch and Sebastian Ewert - 2013 - The Audio Degradation Toolbox and Its Application .pdf:application/pdf}
}

@article{casey_content-based_2008,
	title = {Content-{Based} {Music} {Information} {Retrieval}: {Current} {Directions} and {Future} {Challenges}},
	volume = {96},
	shorttitle = {Content-{Based} {Music} {Information} {Retrieval}},
	doi = {10.1109/JPROC.2008.916370},
	abstract = {The steep rise in music downloading over CD sales has created a major shift in the music industry away from physical media formats and towards online products and services. Music is one of the most popular types of online information and there are now hundreds of music streaming and download services operating on the World-Wide Web. Some of the music collections available are approaching the scale of ten million tracks and this has posed a major challenge for searching, retrieving, and organizing music content. Research efforts in music information retrieval have involved experts from music perception, cognition, musicology, engineering, and computer science engaged in truly interdisciplinary activity that has resulted in many proposed algorithmic and methodological solutions to music search using content-based methods. This paper outlines the problems of content-based music information retrieval and explores the state-of-the-art methods using audio cues (e.g., query by humming, audio fingerprinting, content-based music retrieval) and other cues (e.g., music notation and symbolic representation), and identifies some of the major challenges for the coming years.},
	number = {4},
	journal = {Proceedings of the IEEE},
	author = {Casey, M. A. and Veltkamp, R. and Goto, M. and Leman, M. and Rhodes, C. and Slaney, M.},
	month = apr,
	year = {2008},
	keywords = {Q1, High priority, Audio Feature Extraction, Audio Music Information Retrieval},
	pages = {668--696},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\nesto\\Zotero\\storage\\4ZWJGFFJ\\4472077.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\nesto\\Zotero\\storage\\BHV49ALD\\Casey et al. - 2008 - Content-Based Music Information Retrieval Current.pdf:application/pdf}
}

@inproceedings{franck_performance_2011,
	title = {Performance {Evaluation} of {Algorithms} for {Arbitrary} {Sample} {Rate} {Conversion}},
	url = {http://www.aes.org/e-lib/online/browse.cfm?elib=16078},
	abstract = {Arbitrary sample rate conversion (ASRC) enables changes of the sampling frequency by flexible, time-varying ratios. It can be utilized advantageously in many applications of audio signal processing. Consequently, numerous algorithms for ASRC have been proposed. However, it is often difficult to choose a minimal-cost algorithm that meets the requirements of a specific application. In this paper, several approaches to ASRC are reviewed. Special emphasis is placed on algorithms that enable...},
	language = {English},
	urldate = {2019-09-16},
	booktitle = {Audio {Engineering} {Society} {Convention} 131},
	publisher = {Audio Engineering Society},
	author = {Franck, Andreas},
	month = oct,
	year = {2011},
	keywords = {Q1, Audio Degradation},
	file = {Franck - 2011 - Performance Evaluation of Algorithms for Arbitrary.pdf:C\:\\Users\\nesto\\Zotero\\storage\\EETYI6U3\\Franck - 2011 - Performance Evaluation of Algorithms for Arbitrary.pdf:application/pdf;Snapshot:C\:\\Users\\nesto\\Zotero\\storage\\AFPG9KRA\\browse.html:text/html}
}

@article{downie_scientific_2004,
	title = {The {Scientific} {Evaluation} of {Music} {Information} {Retrieval} {Systems}: {Foundations} and {Future}},
	volume = {28},
	issn = {0148-9267},
	shorttitle = {The {Scientific} {Evaluation} of {Music} {Information} {Retrieval} {Systems}},
	url = {http://dx.doi.org/10.1162/014892604323112211},
	doi = {10.1162/014892604323112211},
	number = {2},
	urldate = {2019-09-16},
	journal = {Comput. Music J.},
	author = {Downie, J. Stephen},
	month = jun,
	year = {2004},
	keywords = {Q1, High priority, Evaluation of Music Information Retrieval},
	pages = {12--23}
}

@incollection{mitrovic_features_2010,
	title = {Features for {Content}-{Based} {Audio} {Retrieval}},
	volume = {78},
	booktitle = {Advances in computers},
	publisher = {Elsevier},
	author = {Mitrović, Dalibor and Zeppelzauer, Matthias and Breiteneder, Christian},
	year = {2010},
	keywords = {Audio Feature Extractor, High priority, Q1},
	pages = {71--150},
	annote = {Summary
 
This paper provides a detailed taxonomy of audio features.
 
In the first level of the taxonomy, there are 6 types of features:

Temporal domain
Frequency domain
Cepstral domain
Modulation frequency domain
Eigendomain
Phase space

 
Within these groups, chroma-based features and tonality are located inside the frequency domain features.
 
Frequency domain features are divided in two groups:

Physical
Perceptual

 
Perceptual frequency domain features consist of the following:

Brightness
Tonality
Loudness
Pitch
Chroma
Harmonicity

 },
	file = {Full Text:C\:\\Users\\nesto\\Zotero\\storage\\XPWYGYK4\\Mitrović et al. - 2010 - Features for content-based audio retrieval.pdf:application/pdf;Snapshot:C\:\\Users\\nesto\\Zotero\\storage\\4BGRJ3LR\\S0065245810780037.html:text/html}
}

@article{lindsay_mpeg-7_2001,
	title = {{MPEG}-7 and {MPEG}-7 {Audio}'{An} {Overview}},
	volume = {49},
	url = {http://www.aes.org/e-lib/browse.cfm?elib=10186},
	abstract = {For more than 10 years the term MPEG (Moving Picture Experts Group) has been synonymous with successful standardization in the field of audiovisual coding. The well-known standards MPEG-1, MPEG-2, and MPEG-4 have defined the state of the art in the perceptual coding of multimedia content. More recently the MPEG standards group (ISO/IEC JTC1/SC29/WG11) has extended its traditional scope by initiating the MPEG-7 standardization process, which aims to define a unified interface for the...},
	language = {English},
	number = {7/8},
	urldate = {2019-09-17},
	journal = {Journal of the Audio Engineering Society},
	author = {Lindsay, Adam T. and Herre, Jürgen},
	month = jul,
	year = {2001},
	keywords = {Audio Feature Extractor, Q1},
	pages = {589--594},
	file = {Snapshot:C\:\\Users\\nesto\\Zotero\\storage\\TUR39GUC\\browse.html:text/html}
}