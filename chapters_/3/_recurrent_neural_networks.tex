% Taken verbatim from comps Q9

% Certain phenomena (music, for example) have a strong
% dependence on time.

% Explaining the meaning of a given input (for example, a
% note) does not only depend on the input, but on the time
% where that input occurs.

% In the research related with Artificial Neural Networks,
% researchers have tried to investigate these type of
% phenomena using Recurrent Neural Networks (RNN).

% An RNN is trained with sequences of inputs, rather than
% just inputs.

% They are useful for many different problems.

Recurrent Neural Networks (RNNs) are a special type of ANNs
(see Question 2 for more information on ANNs) where the
inputs are not considered to be independent but part of a
sequence. This distinction allows the network to model
time-varying processes and tasks that require
sequence-to-sequence models (e.g., language, music, and
weather). An additional benefit of RNN archictures is that,
unlike other types of deep learning networks such as
Convolutional Neural Networks (CNNs), RNNs can process
inputs of arbitrary length. This has made CNNs the most
robust choice for fixed-size grid-like data (e.g., images)
and RNNs a popular choice for tasks where the length of the
input is unknown.

Although modern RNNs have achieved state-of-the-art accuracy
in many tasks that involve sequential inputs, this was not
easily achieved and required many years of innovations. The
most important aspect in which RNNs have seemed to take more
time to progress in comparison to other deep learning
techniques, is the successful training of the networks,
which seemed to be very unstable and remains one of the
biggest challenges of research in RNNs.

\guide{Training RNNs}

Over the years, RNNs have gained a reputation as
architectures that are very difficult to train
\parencite{pascanu2013difficulty}, given that many attempts
of training such models have failed. For example, in 1994,
experiments showed that as the span of dependencies that
need to be captured by the RNN increases, the probability of
successfully training the network via the Stochastic
Gradient Descent (SGD) optimizer rapidly reaches 0 for
sequences of 10 or 20 time steps
\parencite{bengio1994learning}.

The main difficulty of training an RNN is something known as
the \emph{vanishing} and \emph{exploding} gradients. When
the gradients vanish---something very common---the network
is unable to learn long-term dependencies and can only model
the dependencies of inputs that were provided a few time
steps in the past (rather than modelling the entire
sequence, which is usually the goal). When the gradients
explode---something less common but more damaging---the
convergence of the training is compromised because the large
value of the gradient occludes the search of a local minimum
by the optimizer algorithm.

One of the reasons why RNNs are more prone to vanishing and
exploding gradients is their heavy reliability on
\emph{parameter sharing}. Other architectures (e.g. CNNs)
also share parameters between different neurons in order to
reduce the number of parameters and, hence, the effort of
training the network. RNNs, however, make a heavier use of
parameter sharing, as their input is usually a fixed-size
vector designed to receive the time steps of the input
sequence, one at a time. This design decision implies two
things: 1) the parameters of the network are shared for
every input of the sequence, 2) the parameters need to be
updated for every time step of every input sequence.

The second of these design decisions is what mostly
contributes to the vanishing and exploding gradients. In
other architectures, it is expected that the parameters will
be updated once per training example, while in RNNs the
parameters are updated several times per training example.

When the RNN is updating its shared parameters, it is very
easy that these parameters grow out of control (explode) or
drop to zero really fast (vanishing), similar to how a
scalar number would explode or vanish if raised by a very
large exponent (e.g., $0.5^{1000}$ or $10^{1000}$).

By making use of modern solutions, however, RNNs have been
able to successfully learn long-term dependencies (in the
order of hundreds or thousands of time steps
\parencite{hochreiter1997long}), making them practical for
many tasks. There have been many proposed solutions
\parencite{elhihi1995hierarchical, yildiz2012revisiting,
jaeger2012long}, however, the most successful ones are the
\emph{gated RNNs}. Particularly, a type of gated RNN known
as the Long Short-Term Memory (LSTM) architecture.


\guide{Designing an LSTM for detecting modulation}

In order to design a system for the detection of
``modulations'', the first step is to define the
expectations of such a system in terms of its inputs and
outputs. Particularly, given that the term ``modulation'' is
very difficult to define, I will consider referring to it as
a \emph{local key} finder instead (see Question
\ref{chap:chap6} for a further discussion on modulations and
local keys).

A local key-finding system produces an output at every time
step. The output consists of one of the 24 major and minor
keys. That is, the local key for any given time step. The
time step units consist of onset events in the symbolic
music input (i.e., attacks). The inputs of the system could
be the range of valid MIDI note numbers (0-127) in a one-hot
encoding scheme, yielding a fixed input vector of dimension
128 for every time step. Nevertheless, given that some
local-key-finding models have been successfully implemented
without octave information
\parencite{napoleslopez2019keyfinding}, the input vector can
be substituted by a 12-dimensional pitch-class vector in a
one-hot encoding scheme. This reduces the number of
parameters by an order of magnitude and, therefore, is
expected to reduce the computational cost of training the
model.

The system consists of a single hidden layer with a
recurrent LSTM unit. The gates of the LSTM unit use a
sigmoid function as their non-linear activation function and
the input to the network uses a \emph{Rectified linear unit}
(ReLU) as its activation function.

As a proof of concept and given the scarce data available
for training the network, an existing probabilistic model
that requires no training
\parencite{napoleslopez2019keyfinding} is used for
generating local key annotations in a corpus of 892 MIDI
files of classical music. The success criteria of this LSTM
system is, therefore, to provide a similar local-key
segmentation on unseen MIDI examples as the segmentation
provided by the baseline, probabilistic model.
