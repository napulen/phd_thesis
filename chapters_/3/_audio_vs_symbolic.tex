\guide{Difference between audio and symbolic music representations}

Among the differences between the audio and symbolic music
representations, we can find the following
\parencite{fremerey2009towards}:

\textbf{Repetition}: repetitions may be encoded semantically
in symbolic music representations (e.g., using a repetition
barline), whereas they should be explicit in audio
representations.

\textbf{Ambiguity}: some of the elements used in symbolic
music representations present ambiguities in their note and
duration values, for example, arpeggios, trills, grace
notes, and other ornaments. If they exist in audio, they are
explicit.

\textbf{Tempo}: the tempo in symbolic music representations
is often fixed and predictable, particularly when the
symbolic music representation comes from a digital
transcription and not a real performance (e.g., from a
person playing a MIDI controller). In audio recordings,
tempo will most likely have deviations and changes
throughout the piece, particularly when the recording
corresponds to a human performer playing an acoustic
instrument.

\textbf{Loudness}: frequently, the variation in loudness is
much greater in audio representations than it is in symbolic
music representations. Although a guideline of the loudness
of a particular piece can be provided in symbolic
representations through the annotation of dynamics (e.g.,
pianissimo, fortissimo), this is unable to characterize the
level of detail in which a recorded performance could vary
in loudness.

\textbf{Timbre}: different timbre has a significant effect
in the audio representation of a musical signal,
particularly, in its spectral representation. This is one of
the most important distinctions between audio and symbolic
music representations. Although a clue of the timbre can be
provided in symbolic representations through the encoding of
the instrument that is expected to perform the piece, timbre
information is otherwise inexistent in symbolic music
representations.

The list presented here is not an exhaustive list of
differences between audio and symbolic music
representations, but it should give an idea of why
researchers have opted to design models tailored towards
musical inputs in a specific representation. In the next
section, the relevant information for finding musical keys
in each representation is discussed.

\guide{Relevant information for finding musical keys}

In symbolic representations, the input that is typically
sent to a key-finding algorithm consists of a sequence of
either:
\begin{itemize}
    \item Pitches and their durations (e.g., a quarter note
    C4), when working with Humdrum(**kern), MEI, MusicXML,
    and other formats or
    \item Pitch class with octave and a form of time offset
    information (e.g., MIDI note number 60 with delta time
    of 0.5s since the last event), when working with MIDI
    files
\end{itemize}
 All of this information being directly available from the
 symbolic music representation.

In audio representations, on the other hand, the most
popular feature used by key-finding algorithms (see Question
\ref{chap:chap3} for a list of models that make use of this
feature) is the \emph{Chromagram}.\footnote{They are also
referred as \emph{chroma features}, Harmonic Pitch Class
Profiles (HPCP), and a few other names}

\guide{Chromagrams in audio key-finding}

There are a few exceptions of key-finding algorithms that
work with different features, for example, a log-magnitude
log-frequency spectrogram
\parencite{korzeniowski2018genreagnostic} or a spatial
representation of tonality \parencite{arndt2008circular,
harte2006detecting, lee2007unified}. For the most part,
audio key-finding algorithms use a chromagram as their
input.

Regarding chromagrams and the problems they may introduce to
a key-finding system, Campbell says
\parencite{campbell2010automatic}: \emph{[...] most of the
errors encountered during the feature extraction stage [of a
key-finding algorithm] can be attributed to problems with
the pitch-class generation algorithm.} A pitch class
generation algorithm, in this case, is a chromagram
extractor. He considers that these issues can be related to
tuning variations, low frequency resolution, and the effects
of partials.\footnote{In the paper, Campbell credits Chuan
and Chew (2005) for the taxonomy of issues related with
chromagrams, however, the taxonomy does not appear in the
referenced Chuan and Chew (2005) paper}

\guide{Issues introduced by chromagram features}

% \guide{Campbell (2010) and issues with chromagrams}
\begin{enumerate}
    \item Tuning variations: The tuning in an audio
    recording may fluctuate due to the use of a different
    tuning system (e.g., different from the $A4=440Hz$
    standard tuning), a different temperament (common in
    audio recordings of baroque music), or simply because
    the instruments in the recording are out-of-tune. A
    chromagram extractor that does not account for these
    issues may lead to inaccurate pitch-class distributions.
    \item Low frequency resolution: Similar to how MFCCs (by
    the use of the Mel scale), log-magnitude log-frequency
    spectrograms, and other features attempt to take human
    perception into consideration when extracting the
    features, a good chromagram extractor should account for
    the fact that the energy levels at different frequency
    ranges should contribute differently to the output
    pitch-class distribution.
    \item Effect of partials: A good chromagram extractor
    should account for the contribution of partials that are
    not exactly harmonic to the fundamental frequency of a
    playing instrument, but that they are perceived as part
    of it.
\end{enumerate}

Given that chromagrams are ubiquitous in many MIR tasks that
involve the analysis of tonal features, most of these issues
have already been addressed by researchers studying the
design of chromagram feature extractors. One such example of
increasing the robustness of a chromagram feature, is the
proposal by Bello and Pickens \parencite{bello2005robust}.

\guide{Robusting the chromagram}

\guide{Hand-crafted improvements}
The approach from Bello and Pickens proposes a mid-level
harmonic-and-rhythmic representation, akin to a chromagram
\parencite{bello2005robust}. They describe the process for
generating such representation starting from a polyphonic
music signal.

Their approach consists of four stages: computing a 36-bin
chromagram, tuning the chromagram, perform beat-synchronous
(tactus) segmentation, and finally, producing a 12-bin
chromagram reduction. The last stage can be used as a pitch
class distribution for different classification models.

In order to compute the initial 36-bin chromagram, they
process the signal by computing a constant-Q transform,
extracting the chroma bins from the constant-Q spectrum. In
their experiments, they downsampled the signal to 11,025Hz,
analyzing frequencies ranging from $98Hz$ to $5250Hz$.

The reason for using a 36-bin chromagram is to account for
mistuning in the original signal. Bello and Pickens consider
that if a note is mistuned in the original signal, having a
higher number of chroma bins will help the model to decide
to which bin the pitch-class should go to. To accomplish
this tuning process, they build on top of the algorithm
proposed by Harte and Sandler
\parencite{harte2005automatic}.

The next stage consists in the beat-synchronous
segmentation. The motivation for this stage is to overcome
the problems introduced by transient components of the
signal like drums, guitar strumming, and short vocal
ornamentations. For this stage, they build on top of an
external beat-tracking algorithm.

After this stage, the chromagram is averaged and reduced
from 36 bins to 12 bins, adding up the energy at the chroma
bins and collapsing them into 12 semitones.

Although the techniques for hand-crafting chromagrams (such
as this one) have become increasingly better (a survey
provided by Cho and Bello \parencite{cho2014relative}), in
2016, Korzeniowski and Widmer proposed to increase the
robustness of a chromagram extractor through the use of deep
learning.

\guide{Deep-learning-based chromagram extractor}
Korzeniowski and Widmer propose a new type of chromagram
extractor based on artificial neural networks
\parencite{korzeniowski2016feature}. The chromagram is
trained to compute chroma features that encode harmonic
information important for chord recognition tasks but that
learns to ignore irrelevant interferences in the signal. The
training process is achieved by feeding an audio spectrum
with harmonic context (a chord label) instead of a regular
spectral frame.

Korzeniowski and Widmer claim that a chromagram derived from
ground truth annotations is able to recognize 97\% of the
major and minor chords in the Beatles dataset, even using a
simple logistic regression classifier. This motivated them
to create a chromagram extractor that learns to recognize
the relevant harmonic information and be robust to spectral
content that is harmonically irrelevant.

The motivation for using neural networks is that, even if
the hand-crafted improvements to the chromagram extractors
have made them very robust throughout the years,
hand-crafting methods for all conceivable disturbances in
the signal is unfeasible. Therefore, a data-driven approach
could be more useful.

Korzeniowski and Widmer formulated the machine learning task
of their approach in the following way: given the
spectrogram and a chord label, the network needs to produce
the chroma representation.

Their deep neural network architecture consisted of
feedforward layers: 3 hidden layers with 512 units and a
\emph{ReLU} activation function each, and at the output, a
sigmoid-activated layer with 12 units.

In their experiments, their deep learning chromagram
extractor, which they called \emph{deepchroma}, was able to
outperform the state-of-the-art baseline methods in a chord
recognition experiment. This showed that chromagrams not
only remain to be useful features, they can also benefit
from the improvements of deep learning algorithms.

\guide{In search of a different kind of robustness}

Although the previous section has discussed the relevance
and robustness of chromagrams in different chord and
key-finding algorithms, it has not directly addressed some
of the issues presented at the beginning of this chapter,
such as the differences between symbolic music
representations and audio music representations. Among those
differences, we identified that symbolic and audio music
representations differ in how they present repetition,
ambiguity (mostly in duration), tempo, loudness, and timbre.

We can assume that a chromagram, like the \emph{deepchroma}
extractor, is the most robust representation that we can
give to a key-finding algorithm for computing the key of an
audio signal. However, if we are striving for the design of
a hybrid key-finding model, that can be trained with the
events of the symbolic data as well as the pitch-class
distributions of the audio chromagram, do the differences
mentioned above between both representations modify the
chromagram in a way that is no longer comparable, in its
key, to the symbolic representation?

Assessing this question requires a different type of
robustness study. For example, one that considers the
robustness of the chromagram predicting the key of the same
piece, given different versions of that piece. One could
design such an experiment by taking a piece in a symbolic
representation and synthesize it through multiple sampling
libraries, with different timbres (but the same
instruments). It is sensible to think that even if the
timbres of the instruments change, the perceived key of the
piece should not change. Other variations to be tested, for
example, could be ``arpeggiating'' every chord in the
symbolic representation, so that the onsets of the notes are
slightly \emph{off-time} in the synthesized audio and,
hence, the chromagram. Such suite of \emph{alterations} to
the audio and symbolic representations, controlled from the
opposite domain, could be used to test the viability of a
hybrid key-finding model. Although no study exists in the
literature that addresses this specific problem (to the best
of our knowledge), a study that explored the robustness of
chroma features to audio degradations could be an initial
reference. Urbano et al. studied the effect that different
audio degradations had in the computation of chroma features
\parencite{urbano2014what}. During their study, they
modified the input audio signal by changing the sampling
rate, compression codec, and bitrate. Correlating the
signals from the different alterations to find whether the
feature extraction had been compromised. A similar study
with a key-finding stage at the end could possibly address
our specific case.

Nevertheless, one also risks that, by modifying the symbolic
source too much (e.g., in our ``arpeggiating'' alteration),
indeed, the alterations could have perceptual effects in the
perceived key of the piece, at which point it would be
unfair to expect that if the algorithm predicts a different
key than the one that we intend it to predict, it should be
considered a mistake. For that kind of assessment, one could
take as a reference the study from Serr\`a et al.
\parencite{serra2008statistical}, who studied the relation
between chroma statistics and human judgments of tonality.

A combination of these approaches could give us insight
about a few things: the robustness of chromagrams to changes
in audio-synthesized symbolic music representations, the
viability of a hybrid key-finding model, and maybe, the most
critical differences between a music score and a real
performance, key-finding-wise.
