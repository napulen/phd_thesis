% Taken verbatim from AugmentedNet

\guide{Dense and recurrent layers}

Two time-distributed dense layers are applied to the concatenated outputs of the convolutional blocks. The dense layers help reducing the number of features before the GRU layers. These have 64 and 32 neurons, respectively.
% All convolutional and dense layers have batch normalization \parencite{ioffe2015batch} before the activation function. Similarly, all convolutional and dense layers use the rectified linear unit (ReLU) as their activation function.

Two bidirectional GRU \parencite{cho2014learning} layers are applied after the second dense layer.
Both GRU layers return outputs at every timestep.
Throughout the entire network, the dimensionality of the timesteps axis remains constant. That is, our input and output sequences have the same length, and the model predicts one Roman numeral label per timestep.
