\chapter{Model}
\label{chap:chap5}

% \begin{quote}
%     Parallel fifths and other familiar creatures.
% \end{quote}
% \clearpage

The AugmentedNet is similar in size and design to the network proposed by Micchi et al. \cite{micchi_not_2020}. The network is characterized by a different design of the convolutional layers, the representation of pitch spelling, and the separation of bass and chroma features inputs into independent convolutional blocks.

\subsection{Inputs}

\textbf{Reference note per timestep.}
The input to the network consists of a sequence of timesteps, which are sampled from the score at symbolically regular note duration values.
In this study, we use the thirty-second note (`demisemiquaver') as this atomic value (i.e.~eight timesteps per quarter note in the score) in order to match the most fine-grained frame sampling seen in previous work. The length of the sequence is set by a fixed number of timesteps. Following Micchi et al., we set that number at 640 frames (or 80 quarter notes) per sequence example.
%Each timestep contains a vector of bass and chroma features.

\textbf{Bass and spelled chroma features.}
We also follow the Micchi et al.~input representation of pitch as a vector containing bass spelling and spelled `chroma features'\footnote{A chroma feature representation is typically a vector of 12 pitch classes, where the activation of each pitch class at a given timestep is indicated in a many-hot encoding fashion (symbolic data), or as continous values (audio data). 
Due to the similarity of this input representation to chroma features, except for the spelling aspect, we refer to them as \emph{spelled} chroma features.} at every timestep, however the details differ in subtle but important ways. 
In the Micchi et al. representation, each timestep has 70 features: 35 for the bass and 35 for the chroma features. 
We consolidate this information in 38 features: 19 for the bass, and 19 for the chroma features. The reduction in number of features is due to an alternative encoding of pitch spelling, described below.

\textbf{Encoding the pitch spelling.}
%In previous work, Micchi et al. \cite{micchi_not_2020} have encoded pitch spellings as a one-hot encoded vector with 35 features, which encodes pitch spellings up to two sharps or two flats of every note letter (7 * 5 = 35). 
We split the representation of a pitch spelling into two components: the pitch class (0--12) and the generic note letter (A--G).
Each spelled pitch thus leads to a two-hot encoded vector with 19 features (1 of 12 pitch classes, and 1 of 7 note names).
This reduces the number of parameters in the network without any observable compromise in performance.

\guide{Separating the inputs.}
Furthermore, the bass and spelled chroma features are connected to the network independently and concatenated only after they have passed through their own convolutional blocks. 
In preliminary experiments, we discovered that this separation of the inputs (bass and spelled chroma features) was beneficial to the learning process.
Thus, two parallel convolutional blocks are computed.

\subsection{Convolutional block}

\guide{Micchi and DenseNet-like convolutions.}
Using the feature maps of all previous layers as an input to a convolutional layer has proven beneficial, for instance by strengthening feature propagation and reducing the number of parameters \cite{huang_densely_2017}. 
Moreover, DenseNet-like architectures have shown to work well for the specific task of functional harmony \cite{micchi_not_2020}.

\begin{figure*}
 \centerline{
 \includegraphics[width=\textwidth]{figs/network.png}}
 \caption{\emph{AugmentedNet}. The bass and chroma inputs are processed through independent convolutional blocks and then concatenated. Both convolutional blocks are identical and expanded on the top of the figure. A convolutional block has six 1D convolutional layers. Each layer doubles the kernel size (number of timesteps covered) and halves the number of output filters, prioritizing short-term dependencies but providing long-term context that benefits the subsequent GRU layers.}
 \label{fig:network}
\end{figure*}

\guide{Convolutional layers.}
We follow similar methods, reusing the feature maps computed for a given timestep in subsequent convolutions. 
Figure \ref{fig:network} provides a schematic diagram of our network, with the convolutional block on the top left area of the figure. 
In our preliminary experiments, we noticed that different tonal tasks of functional harmony have different time dependencies. 
For example, losing information about a specific timestep often leads to poor performance in predicting the inversion, whereas losing long-term dependencies hinders performance in local key estimation. 
Our architecture implicitly prioritizes short-term dependencies in the initial convolutional layers, by having more filters and covering less timesteps. Going further, the convolutions provide more context about future timesteps, but output a smaller number of filters. These increments (in kernel size) and decrements (in filters) are done in powers of 2. With a configuration of 6 convolutional layers in each block (as shown in Figure \ref{fig:network}), the first layer convolves a single timestep (`demisemiquaver') and the sixth layer convolves 32 timesteps (`whole note'). The output shape of the block is the original length of the sequence with 82 features per timestep.

\subsection{Dense and recurrent layers}

Two time-distributed dense layers are applied to the concatenated outputs of the convolutional blocks. The dense layers help reducing the number of features before the GRU layers. These have 64 and 32 neurons, respectively. 
% All convolutional and dense layers have batch normalization \cite{ioffe_batch_2015} before the activation function. Similarly, all convolutional and dense layers use the rectified linear unit (ReLU) as their activation function.

Two bidirectional GRU \cite{cho_learning_2014} layers are applied after the second dense layer. 
Both GRU layers return outputs at every timestep. 
Throughout the entire network, the dimensionality of the timesteps axis remains constant. That is, our input and output sequences have the same length, and the model predicts one Roman numeral label per timestep. 

\subsection{Multi-outputs}

The output of the network follows a MTL approach with hard parameter sharing, similar to that proposed by Chen and Su \cite{chen_functional_2018}. For each of the output tasks, a time-distributed dense layer is attached to the second GRU, and used to predict its corresponding task. 
Six conventional tasks are learned (similar to Micchi et al. and Chen and Su \cite{micchi_not_2020, chen_attend_2021}), plus five additional ones. All the tasks (eleven in total) and their number of output classes are shown on the right side of Figure \ref{fig:network}.

\subsubsection{Six conventional functional harmony tasks}
All the conventional tasks, except for the local key, have the same number of output classes described by Micchi et al. \cite{micchi_not_2020}. The local key includes four additional keys: $\{F\flat, G\sharp, d\flat, e\sharp\}$. These were included because the data exploration process revealed modulations reaching $G\sharp$ major in the dataset.
Thus, the number of allowed key signatures was extended by one sharp and one flat, in both modes.
% This also introduced more training examples during the data augmentation process via key transposition. 

\subsubsection{Five Additional tasks.}\label{sec:additionaltasks}

It is argued that MTL helps improving the generalization of a model by preferring representations that are useful to related tasks, acting as an implicit form of data augmentation and regularization method \cite{ruder_overview_2017}.
Roman numeral labels can be decomposed into multiple different features, of which the six conventional tasks are known examples.
Motivated by the possibility of improving the performance of our network, we included five additional tasks with hard parameter sharing in our MTL approach.
We describe the five additional tasks, which have relevance to harmonic analysis. 
One of them, RomanNumeral75, was also used as an alternative task when reconstructing the final Roman numeral label, a process we explain below.

\begin{table}[]
\begin{tabular}{l|l|l|l|l}
1--15    & 16--30    & 31--45    & 46--60     & 61--75    \\
\hline
I       & V/V      & Ger     & viio7/v   & V+       \\
V7      & v        & N        & viio7/iii & viio/vi  \\
V       & V7/ii    & viio7/vi & IV/V      & III+     \\
i       & III      & V/ii     & I+        & V/iii    \\
IV      & iiø7     & viiø7    & I7        & ii/V     \\
ii      & iii      & V9       & viio/IV   & I/bVI    \\
vi      & iio      & viio/ii  & V/III     & viio7/IV \\
iv      & viio/V   & V/iv     & V7/iii    & V7/v     \\
viio7   & V7/vi    & Cad/V    & viio/iv   & i7       \\
viio    & VII      & iv7      & iio7      & iii7     \\
V7/V    & viio7/ii & viio7/iv & VI7       & Fr      \\
V7/IV   & I/V      & IV7      & I/III     & V/IV     \\
viio7/V & V7/iv    & V7/III   & V7/VI     & vii      \\
VI      & V/vi     & viiø7/V  & bVII      & V/v      \\
ii7     & vi7      & It       & bVI       & II      
\end{tabular}
\caption{The 75 most-common Roman numeral strings, where the inversion has been removed and learned independently. During data exploration, these classes spanned 98\% of the Roman numeral annotations across all the annotated data. Predicting these classes is an alternative to predicting the chord root, chord quality, primary degree, and secondary degree simultaneously.}
\label{tab:top75rn}
\end{table}

\textbf{RomanNumeral75}. During data exploration, we found that, when inversions were removed and synonyms (e.g., \texttt{bII6} and \texttt{N6}) were standardized, a set of 75 Roman numeral strings spanned approximately $98\%$ of all the annotations across all datasets. This was a motivation to predict the Roman numeral string itself. The correct prediction of this task is equivalent to predicting the chord root, chord quality, primary degree, and secondary degree simultaneously. As an additional experiment in our results section, we substituted the final reconstruction of the Roman numeral label in this fashion, using the local key, the inversion, and the RomanNumeral75 outputs. The performance with this method is always better than through the four conventional tasks. The 75 classes of common Roman numeral strings are shown in Table \ref{tab:top75rn}. 


\textbf{Harmonic Rhythm.} Whether a Roman numeral annotation label starts at the given timestep. A binary classification task that may be relevant for chord segmentation.

\textbf{Bass}. The bass note implied by the Roman numeral label. This is predicted with 35 output classes, similarly to the chord root. The 35 classes represent a spelled pitch. This task is related to predicting the chord inversion.

\textbf{Tonicization}. The tonicization is predicted from the key implied by a secondary degree (if any). 
% When not present, the tonicization encodes the local key instead, to reduce the sparsity of classes. 
% This idea is adapted from N\'apoles L\'opez et al. \cite{napoles_lopez_local_2020}. 
The output classes are identical to the ones of the local key and is an alternative approach to learn the secondary degrees.

\textbf{Pitch Class Sets}. The set of pitch classes implied by the chord. The number of classes (93) results from computing all pitch class sets in all diatonic triad and seventh chords, plus all augmented sixth chords in all keys. This task is related to the chord quality, primary degree, and to non-chord tones \cite{ju_non-chord_2017}.
All additional tasks except for the RomanNumeral75 are computed solely for their contributions to the shared representation of the MTL approach.

\subsection{Data augmentation}

\subsubsection{Transposition}

As in most automatic tonal music analysis research, we transpose each piece to different keys as a means for data augmentation. 
Particularly, we transpose to all the keys that lie within a range of key signatures in both modes.
When we transpose a piece, we verify that all the modulations within the piece fall in the target range of key signatures.
This process of transposition and data augmentation was introduced and described by Micchi et al. \cite{micchi_not_2020}.
In our data exploration, we found G$\sharp$ major to be the furthest key to the center of the \emph{line-of-fifths} \cite{temperley_line_2000} in the dataset. 
Thus, we transposed in the range of keys within 8 flats to 8 sharps in their key signature.
% Due to modulations, two pieces of music may be transposed a different number of times. 
% Generally, chromatic pieces (passing through many different keys) can be transposed to fewer new keys, while more diatonic pieces (which remain in one or few key/s) can be transposed more.

\subsubsection{Synthetic examples}

\guide{Additional data augmentation.}
In addition to transposition, we implemented a variation of a recent data-augmentation technique proposed by N\'apoles L\'opez and Fujinaga \cite{napoles_lopez_harmonic_2020}.
Starting with the Roman numeral analyses of our dataset, we synthesized `new' training examples by realizing the chords implied by each Roman numeral annotation.
The synthesis was done using the music21 Python library \cite{cuthbert_music21_2010}, which converts RomanText \cite{gotham_romantext_2019} files into scores of \emph{block chord} realizations.

\begin{figure}
 \centerline{
 \includegraphics[width=\columnwidth]{figs/texturization.png}}
 \caption{Example of texturization. The \emph{block chord} texture (b) was synthesized using music21 \cite{cuthbert_music21_2010} from an input RomanText file \cite{gotham_romantext_2019}. The texturized output (c) was generated by recursively applying musical patterns to the \emph{block chord} scores. The three musical patterns of \emph{bass-split}, \emph{Alberti bass}, and \emph{syncopation} are indicated in measures 1--3, respectively. The original music score (a) is shown for reference: mm. 1--4 of Beethoven's Piano Sonata Op.2 No.1.}
 \label{fig:texturization}
\end{figure}

We found the default \emph{block chord} texture of the synthetic examples to be only slightly beneficial for the model, possibly because it did not capture the complex texture of real keyboard music, for example.
In order to account for this difference, we artificially ``texturized'' the generated training examples, departing from the default \emph{block chords}.
The texturization process involved applying three simple note patterns recursively.
Figure \ref{fig:texturization} shows an example of the texturization patterns, alongside the original score and the default \emph{block chords} provided by music21. We describe the texturization patterns below.

\textbf{Bass-split (measure 1).} The original chord duration is divided by half, playing
the bass note in isolation during the first half, followed by the remaining upper notes.

\textbf{Alberti bass (measure 2).} A 4-note melodic pattern with the contour lowest, highest, middle, highest.

\textbf{Syncopation (measure 3).} The highest note is played in isolation, followed by the remaining lower notes, played in syncopation.

\textbf{Mixture (measure 4).} We applied these patterns randomly and recursively. For example, the \emph{mixture} in measure 4 displays a \emph{bass-split} pattern over the whole-note chord, followed by a \emph{syncopation} pattern applied over the three upper notes, in the second half of the measure. 

As part of the randomization, some chords were left unaltered (e.g., the anacrusis of Figure \ref{fig:texturization}), and the patterns were applied across different duration values. 
To constraint the depth of the recursion, we applied these patterns only to the slices of the score that contained 3--4 simultaneous notes.  
This process resulted in the generation of `new' pieces that showed improvements in the learning process of the model, further the than \emph{block chord} synthetic scores.

\bibliographystyle{plain}
\bibliography{zoterorefs}